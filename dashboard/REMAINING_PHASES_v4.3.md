# ğŸ“‹ Dashboard ì”ì—¬ Phase ê°œì„  ê³„íš (v4.3 â†’ v5.0)

> **í”„ë¡œì íŠ¸:** Slurm Cluster Management Dashboard
> **í˜„ì¬ ë²„ì „:** v4.3.0 (Phase 3 ì™„ë£Œ)
> **ëª©í‘œ ë²„ì „:** v5.0
> **ì‘ì„±ì¼:** 2025-11-05
> **Phase 1-3 ì™„ë£Œ:** Apptainer Discovery, Template Management, File Upload API

---

## âš ï¸ ê°œë°œ ê·œì¹™ ë° ê°€ì´ë“œë¼ì¸

### ğŸ”’ í•µì‹¬ ì›ì¹™ (MUST FOLLOW)

#### 1. ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥
- âœ… **ê¸°ì¡´ ì‹œìŠ¤í…œ ë³´í˜¸**: í˜„ì¬ ì˜ ë™ì‘í•˜ëŠ” ì‹œìŠ¤í…œì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ìµœëŒ€í•œ ì£¼ì˜
- âœ… **ì ì§„ì  ê°œì„ **: í•œ ë²ˆì— í•˜ë‚˜ì˜ ê¸°ëŠ¥ë§Œ ìˆ˜ì •í•˜ê³  ì² ì €íˆ í…ŒìŠ¤íŠ¸
- âœ… **ë¡¤ë°± ê°€ëŠ¥ì„±**: ëª¨ë“  ë³€ê²½ì‚¬í•­ì€ ë¡¤ë°± ê°€ëŠ¥í•˜ë„ë¡ ë°±ì—… ìœ ì§€
- âœ… **ì˜ì¡´ì„± ìµœì†Œí™”**: ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ ê¸°ì¡´ ê¸°ëŠ¥ì— ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ ë…ë¦½ì ìœ¼ë¡œ ì„¤ê³„

#### 2. ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° í•´ê²°
- âŒ **ì„ì‹œë°©í¸ ê¸ˆì§€**: "ë¹¨ë¦¬ ëŒì•„ê°€ê²Œ" í•˜ëŠ” ì„ì‹œ í•´ë²• ê¸ˆì§€
- âœ… **ê·¼ë³¸ ì›ì¸ ë¶„ì„**: ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ê³  ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°
- âœ… **ë¬¸ì„œí™”**: ë¬¸ì œ ë°œìƒ ì›ì¸ê³¼ í•´ê²° ë°©ë²•ì„ ìƒì„¸íˆ ë¬¸ì„œí™”
- âœ… **ì¬ë°œ ë°©ì§€**: ë™ì¼í•œ ë¬¸ì œê°€ ë‹¤ì‹œ ë°œìƒí•˜ì§€ ì•Šë„ë¡ êµ¬ì¡°ì  ê°œì„ 

#### 3. ì†ŒìŠ¤ ì½”ë“œ ê¸°ë°˜ ìˆ˜ì •
- âŒ **ìš´ì˜ ì„œë²„ ì§ì ‘ ìˆ˜ì • ê¸ˆì§€**: ë°°í¬ëœ ì„œë²„ì˜ íŒŒì¼ì„ ì§ì ‘ ìˆ˜ì •í•˜ì§€ ë§ê²ƒ
- âœ… **ì†ŒìŠ¤ ì½”ë“œ ìˆ˜ì •**: ë¹Œë“œ ì „ ì†ŒìŠ¤ ì½”ë“œ ìˆ˜ì • (frontend_3010, backend_5010, websocket_5011)
- âœ… **Setup ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •**: ë°°í¬ ê³¼ì • ë³€ê²½ì€ setup ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (phase*.sh)
- âœ… **ë²„ì „ ê´€ë¦¬**: ëª¨ë“  ë³€ê²½ì‚¬í•­ì€ Gitìœ¼ë¡œ ê´€ë¦¬

#### 4. ìë™ ë°°í¬ ì‹œìŠ¤í…œ í†µí•©
- âœ… **setup_cluster_full_multihead.sh í†µí•©**: ëª¨ë“  ë³€ê²½ì‚¬í•­ì´ ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ì— ë°˜ì˜
- âœ… **ë©±ë“±ì„± ë³´ì¥**: ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ë™ì¼í•œ ê²°ê³¼ ë³´ì¥
- âœ… **ì—ëŸ¬ í•¸ë“¤ë§**: ê° Phaseì—ì„œ ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ exit code ë°˜í™˜ ë° ë¡¤ë°±
- âœ… **ì˜ì¡´ì„± ê²€ì¦**: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì „ í•„ìˆ˜ ì˜ì¡´ì„± ê²€ì¦

#### 5. ì‹ ê·œ ì„œë²„ ë°°í¬ ëŒ€ì‘
- âœ… **í—¤ë“œë…¸ë“œ ë…ë¦½ì„±**: í—¤ë“œë…¸ë“œ ì„¤ì •ë„ setup íŒŒì¼ì— í¬í•¨í•˜ì—¬ ìë™í™”
- âœ… **í™˜ê²½ ì„¤ì • íŒŒì¼í™”**: í•˜ë“œì½”ë”©ëœ ê²½ë¡œ/í¬íŠ¸/IP ëŒ€ì‹  í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ ì‚¬ìš©
- âœ… **ì˜ì¡´ì„± ìë™ ì„¤ì¹˜**: í•„ìš”í•œ íŒ¨í‚¤ì§€, ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜
- âœ… **ì´ˆê¸° ë°ì´í„° ìƒì„±**: DB ì´ˆê¸°í™”, ìƒ˜í”Œ ë°ì´í„°, ê¸°ë³¸ í…œí”Œë¦¿ ìë™ ìƒì„±

#### 6. Job Submit ê°œì„  ìš°ì„ ìˆœìœ„
- âš ï¸ **í˜„ì¬ ìƒíƒœ**: Job submitì„ ì œì™¸í•œ ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì€ ì •ìƒ ë™ì‘
- âœ… **ì˜í–¥ ë²”ìœ„ íŒŒì•…**: ìˆ˜ì •ì´ ë‹¤ë¥¸ ì‹œìŠ¤í…œì— ë¯¸ì¹  ì˜í–¥ ì‚¬ì „ ë¶„ì„
- âœ… **ë‹¨ê³„ì  ê²€ì¦**: ê° ë‹¨ê³„ë§ˆë‹¤ Job submit ê¸°ëŠ¥ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦
- âœ… **í†µí•© í…ŒìŠ¤íŠ¸**: Template â†’ File Upload â†’ Job Submit â†’ Monitoring ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸

---

## ğŸ“Š Phase 1-3 ì™„ë£Œ í˜„í™©

### âœ… Phase 1: Apptainer Discovery & Integration
**ì™„ë£Œì¼:** 2025-11-05
**ì£¼ìš” ì„±ê³¼:**
- âœ… `apptainer_service.py` - SSH ê¸°ë°˜ ì´ë¯¸ì§€ ìŠ¤ìº” ì„œë¹„ìŠ¤
- âœ… `apptainer_api.py` - REST API ì—”ë“œí¬ì¸íŠ¸
- âœ… DB ë§ˆì´ê·¸ë ˆì´ì…˜ v4.1.0 - apptainer_images í…Œì´ë¸”
- âœ… `ApptainerSelector.tsx` - Frontend ì´ë¯¸ì§€ ì„ íƒ ì»´í¬ë„ŒíŠ¸
- âœ… ë°°í¬ ì™„ë£Œ ë° API í…ŒìŠ¤íŠ¸ ì„±ê³µ

### âœ… Phase 2: Template Management System
**ì™„ë£Œì¼:** 2025-11-05
**ì£¼ìš” ì„±ê³¼:**
- âœ… `/shared/templates/` - ì™¸ë¶€ YAML í…œí”Œë¦¿ ì €ì¥ì†Œ êµ¬ì¡°
- âœ… `template_loader.py` - í…œí”Œë¦¿ ë¡œë”© ë° DB ë™ê¸°í™”
- âœ… `template_watcher.py` - íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œ (Hot Reload)
- âœ… `templates_api_v2.py` - Template Management REST API
- âœ… DB ë§ˆì´ê·¸ë ˆì´ì…˜ v4.2.0 - job_templates_v2 í…Œì´ë¸”
- âœ… ë°°í¬ ì™„ë£Œ ë° API í…ŒìŠ¤íŠ¸ ì„±ê³µ

### âœ… Phase 3: Unified File Upload API (Backend)
**ì™„ë£Œì¼:** 2025-11-05
**ì£¼ìš” ì„±ê³¼:**
- âœ… `file_classifier.py` - íŒŒì¼ íƒ€ì… ìë™ ë¶„ë¥˜ (7ê°€ì§€ íƒ€ì…)
- âœ… `file_upload_api.py` - ì²­í¬ ê¸°ë°˜ ì—…ë¡œë“œ REST API
- âœ… WebSocket `broadcast_message()` - ì§„í–‰ë¥  ë¸Œë¡œë“œìºìŠ¤íŠ¸
- âœ… DB ë§ˆì´ê·¸ë ˆì´ì…˜ v4.3.0 - file_uploads í…Œì´ë¸”
- âœ… `/shared/uploads/` ì €ì¥ì†Œ êµ¬ì¡° ìƒì„±
- âœ… ë°°í¬ ì™„ë£Œ ë° API í…ŒìŠ¤íŠ¸ ì„±ê³µ (4ê°€ì§€ íŒŒì¼ íƒ€ì… ê²€ì¦)

**âš ï¸ Phase 3 ì”ì—¬ ì‘ì—…:**
- âŒ Frontend ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸ (UnifiedUploader.tsx)
- âŒ ì²­í¬ ì—…ë¡œë“œ UI ë° ì§„í–‰ë¥  í‘œì‹œ
- âŒ WebSocket ì—°ë™ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- âŒ í…œí”Œë¦¿ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ íŒŒì¼ ê²€ì¦ UI

---

## ğŸš€ Phase 3 ì™„ì„±: Frontend File Upload (ìš°ì„ ìˆœìœ„ 1)

**ëª©í‘œ:** Backend APIì™€ ì—°ë™ë˜ëŠ” í†µí•© íŒŒì¼ ì—…ë¡œë“œ UI êµ¬í˜„
**ì˜ˆìƒ ê¸°ê°„:** 3-4ì¼
**ì˜ì¡´ì„±:** Phase 3 Backend ì™„ë£Œ âœ…

### 3.4 Frontend Upload Component êµ¬í˜„

#### íŒŒì¼ êµ¬ì¡°
```
frontend_3010/src/
â”œâ”€â”€ components/
â”‚   â””â”€â”€ FileUpload/
â”‚       â”œâ”€â”€ UnifiedUploader.tsx          # ë©”ì¸ ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸
â”‚       â”œâ”€â”€ ChunkUploader.ts             # ì²­í¬ ì—…ë¡œë“œ ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ FileClassifier.tsx           # íŒŒì¼ ë¶„ë¥˜ UI
â”‚       â”œâ”€â”€ UploadProgress.tsx           # ì§„í–‰ë¥  í‘œì‹œ
â”‚       â””â”€â”€ FilePreview.tsx              # íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useFileUpload.ts                 # íŒŒì¼ ì—…ë¡œë“œ í›…
â”‚   â””â”€â”€ useUploadProgress.ts             # ì§„í–‰ë¥  WebSocket í›…
â””â”€â”€ types/
    â””â”€â”€ upload.ts                        # ì—…ë¡œë“œ íƒ€ì… ì •ì˜
```

#### êµ¬í˜„ ë‚´ìš©

**1. UnifiedUploader.tsx - ë©”ì¸ ì»´í¬ë„ŒíŠ¸**
```typescript
interface UnifiedUploaderProps {
  templateId?: string;                   // í…œí”Œë¦¿ ID (ê²€ì¦ìš©)
  jobId?: string;                        // Job ID (ì €ì¥ ê²½ë¡œ ê²°ì •)
  userId: string;                        // ì‚¬ìš©ì ID
  onComplete: (files: UploadedFile[]) => void;
  onError?: (error: Error) => void;
  maxFileSize?: number;                  // ê¸°ë³¸ 50GB
  maxFiles?: number;                     // ìµœëŒ€ íŒŒì¼ ìˆ˜
  acceptedTypes?: string[];              // í—ˆìš© íŒŒì¼ íƒ€ì…
  enableChunking?: boolean;              // ì²­í¬ ì—…ë¡œë“œ í™œì„±í™”
  chunkSize?: number;                    // ì²­í¬ í¬ê¸° (ê¸°ë³¸ 5MB)
}

export const UnifiedUploader: React.FC<UnifiedUploaderProps> = ({
  templateId,
  jobId,
  userId,
  onComplete,
  onError,
  maxFileSize = 50 * 1024 * 1024 * 1024, // 50GB
  maxFiles = 20,
  acceptedTypes,
  enableChunking = true,
  chunkSize = 5 * 1024 * 1024
}) => {
  // Features:
  // - Drag & Drop ì§€ì›
  // - ë‹¤ì¤‘ íŒŒì¼ ì„ íƒ
  // - íŒŒì¼ íƒ€ì… ìë™ ë¶„ë¥˜ í‘œì‹œ
  // - ì‹¤ì‹œê°„ ì§„í–‰ë¥  (WebSocket)
  // - ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í¬ ì—…ë¡œë“œ
  // - ì—…ë¡œë“œ ì¼ì‹œì •ì§€/ì¬ê°œ/ì·¨ì†Œ
  // - íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
  // - ì¤‘ë³µ íŒŒì¼ ê°ì§€
  // - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
};
```

**2. ChunkUploader.ts - ì²­í¬ ì—…ë¡œë“œ ìœ í‹¸**
```typescript
interface ChunkUploadOptions {
  file: File;
  uploadId: string;
  chunkSize: number;
  onProgress: (progress: number) => void;
  onError: (error: Error) => void;
  signal?: AbortSignal;                  // ì·¨ì†Œ ì§€ì›
}

class ChunkUploader {
  private uploadQueue: Map<string, ChunkUploadState> = new Map();

  async uploadFile(options: ChunkUploadOptions): Promise<void> {
    const { file, uploadId, chunkSize, onProgress, onError, signal } = options;

    const totalChunks = Math.ceil(file.size / chunkSize);
    let uploadedChunks = 0;

    for (let i = 0; i < totalChunks; i++) {
      // ì·¨ì†Œ ì²´í¬
      if (signal?.aborted) {
        throw new Error('Upload cancelled');
      }

      const chunk = file.slice(i * chunkSize, (i + 1) * chunkSize);

      try {
        await this.uploadChunk(uploadId, chunk, i);
        uploadedChunks++;
        onProgress((uploadedChunks / totalChunks) * 100);
      } catch (error) {
        // ì¬ì‹œë„ ë¡œì§ (3íšŒ)
        await this.retryChunk(uploadId, chunk, i, 3);
      }
    }

    // ì—…ë¡œë“œ ì™„ë£Œ ìš”ì²­
    await this.completeUpload(uploadId);
  }

  async uploadChunk(uploadId: string, chunk: Blob, index: number): Promise<void> {
    const formData = new FormData();
    formData.append('upload_id', uploadId);
    formData.append('chunk_index', index.toString());
    formData.append('chunk', chunk);

    const response = await fetch('/api/v2/files/upload/chunk', {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      throw new Error(`Chunk ${index} upload failed`);
    }
  }

  async pauseUpload(uploadId: string): Promise<void> {
    // ì—…ë¡œë“œ ì¼ì‹œì •ì§€
  }

  async resumeUpload(uploadId: string): Promise<void> {
    // ì—…ë¡œë“œ ì¬ê°œ
  }

  async cancelUpload(uploadId: string): Promise<void> {
    // ì—…ë¡œë“œ ì·¨ì†Œ ë° ì •ë¦¬
    await fetch(`/api/v2/files/uploads/${uploadId}`, {
      method: 'DELETE'
    });
  }
}
```

**3. useFileUpload.ts - ì—…ë¡œë“œ í›…**
```typescript
interface UseFileUploadResult {
  uploadFiles: (files: File[]) => Promise<void>;
  uploadProgress: Record<string, number>;
  uploadStatus: Record<string, 'pending' | 'uploading' | 'completed' | 'error'>;
  cancelUpload: (fileId: string) => void;
  pauseUpload: (fileId: string) => void;
  resumeUpload: (fileId: string) => void;
  isUploading: boolean;
  error: Error | null;
}

export const useFileUpload = (options: FileUploadOptions): UseFileUploadResult => {
  const [uploadProgress, setUploadProgress] = useState<Record<string, number>>({});
  const [uploadStatus, setUploadStatus] = useState<Record<string, string>>({});
  const [error, setError] = useState<Error | null>(null);

  const uploadFiles = async (files: File[]) => {
    // 1. ê° íŒŒì¼ì— ëŒ€í•´ ì—…ë¡œë“œ ì„¸ì…˜ ì´ˆê¸°í™”
    for (const file of files) {
      const response = await fetch('/api/v2/files/upload/init', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          filename: file.name,
          file_size: file.size,
          user_id: options.userId,
          job_id: options.jobId
        })
      });

      const { upload_id, chunk_size, total_chunks } = await response.json();

      // 2. ì²­í¬ ì—…ë¡œë“œ ì‹œì‘
      const uploader = new ChunkUploader();
      await uploader.uploadFile({
        file,
        uploadId: upload_id,
        chunkSize: chunk_size,
        onProgress: (progress) => {
          setUploadProgress(prev => ({ ...prev, [upload_id]: progress }));
        },
        onError: setError
      });
    }
  };

  return {
    uploadFiles,
    uploadProgress,
    uploadStatus,
    cancelUpload,
    pauseUpload,
    resumeUpload,
    isUploading: Object.values(uploadStatus).some(s => s === 'uploading'),
    error
  };
};
```

**4. useUploadProgress.ts - WebSocket ì§„í–‰ë¥  í›…**
```typescript
interface UploadProgressEvent {
  upload_id: string;
  uploaded_chunks: number;
  total_chunks: number;
  progress: number;
  status: string;
}

export const useUploadProgress = (uploadId: string) => {
  const [progress, setProgress] = useState(0);
  const [status, setStatus] = useState<string>('pending');

  useEffect(() => {
    const ws = new WebSocket('ws://localhost:5011/ws');

    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);

      if (data.type === 'upload_progress' && data.data.upload_id === uploadId) {
        setProgress(data.data.progress);
        setStatus(data.data.status || 'uploading');
      }
    };

    return () => ws.close();
  }, [uploadId]);

  return { progress, status };
};
```

**5. FileClassifier.tsx - íŒŒì¼ ë¶„ë¥˜ í‘œì‹œ**
```typescript
interface FileClassifierProps {
  files: File[];
  onClassified: (classified: Record<string, File[]>) => void;
}

export const FileClassifier: React.FC<FileClassifierProps> = ({
  files,
  onClassified
}) => {
  const [classified, setClassified] = useState<Record<string, File[]>>({});

  useEffect(() => {
    // íŒŒì¼ ë¶„ë¥˜ (í™•ì¥ì ê¸°ë°˜)
    const groups: Record<string, File[]> = {
      data: [],
      config: [],
      script: [],
      model: [],
      mesh: [],
      result: [],
      other: []
    };

    files.forEach(file => {
      const ext = file.name.split('.').pop()?.toLowerCase();
      const type = detectFileType(ext);
      groups[type].push(file);
    });

    setClassified(groups);
    onClassified(groups);
  }, [files]);

  return (
    <div className="file-classifier">
      {Object.entries(classified).map(([type, typeFiles]) => (
        typeFiles.length > 0 && (
          <div key={type} className="file-group">
            <h3>{type.toUpperCase()} Files ({typeFiles.length})</h3>
            <ul>
              {typeFiles.map(file => (
                <li key={file.name}>
                  {file.name} ({formatSize(file.size)})
                </li>
              ))}
            </ul>
          </div>
        )
      ))}
    </div>
  );
};
```

### 3.5 í†µí•© í…ŒìŠ¤íŠ¸

**ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¨ì¼ ì‘ì€ íŒŒì¼ ì—…ë¡œë“œ**
```typescript
// Test: 1MB íŒŒì¼ ì—…ë¡œë“œ
const smallFile = new File(['test data'], 'config.yaml', { type: 'text/yaml' });
await uploadFiles([smallFile]);
// Expected: ë‹¨ì¼ ì²­í¬, ì¦‰ì‹œ ì™„ë£Œ
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í¬ ì—…ë¡œë“œ**
```typescript
// Test: 10GB íŒŒì¼ ì—…ë¡œë“œ
const largeFile = new File([/* 10GB data */], 'dataset.tar.gz');
await uploadFiles([largeFile]);
// Expected: 2000ê°œ ì²­í¬, ì§„í–‰ë¥  í‘œì‹œ
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ**
```typescript
// Test: ì—¬ëŸ¬ íƒ€ì…ì˜ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ
const files = [
  new File(['data'], 'input.dat'),
  new File(['config'], 'config.json'),
  new File(['script'], 'run.sh')
];
await uploadFiles(files);
// Expected: 3ê°œ íŒŒì¼ ë¶„ë¥˜ ë° ê°ê° ì—…ë¡œë“œ
```

**ì‹œë‚˜ë¦¬ì˜¤ 4: ì—…ë¡œë“œ ì·¨ì†Œ ë° ì¬ê°œ**
```typescript
// Test: ì—…ë¡œë“œ ë„ì¤‘ ì¼ì‹œì •ì§€ í›„ ì¬ê°œ
await pauseUpload(uploadId);
// ... ì‹œê°„ ê²½ê³¼
await resumeUpload(uploadId);
// Expected: ì´ì „ ì²­í¬ë¶€í„° ì¬ê°œ
```

### 3.6 ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `UnifiedUploader.tsx` êµ¬í˜„ ì™„ë£Œ
- [ ] `ChunkUploader.ts` êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] WebSocket ì—°ë™ í…ŒìŠ¤íŠ¸
- [ ] íŒŒì¼ ë¶„ë¥˜ UI í…ŒìŠ¤íŠ¸
- [ ] ì§„í–‰ë¥  í‘œì‹œ í…ŒìŠ¤íŠ¸
- [ ] ëŒ€ìš©ëŸ‰ íŒŒì¼ (5GB+) ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ í…ŒìŠ¤íŠ¸
- [ ] Job Submit í”Œë¡œìš°ì™€ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] Frontend ë¹Œë“œ ë° ë°°í¬
- [ ] ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ E2E í…ŒìŠ¤íŠ¸

---

## ğŸ” Phase 4: Security & Infrastructure (ìš°ì„ ìˆœìœ„ 2)

**ëª©í‘œ:** ë³´ì•ˆ ê°•í™” ë° ì¸í”„ë¼ ìµœì í™”
**ì˜ˆìƒ ê¸°ê°„:** 2ì£¼
**ì˜ì¡´ì„±:** ì—†ìŒ (ë…ë¦½ì  êµ¬í˜„ ê°€ëŠ¥)

### 4.1 JWT Refresh Token System

**ëª©ì :** Access Token ì§§ì€ TTL + Refresh Token ì¥ê¸° TTLë¡œ ë³´ì•ˆ ê°•í™”

**ìƒˆ íŒŒì¼:** `auth_portal_4430/token_service.py`

```python
import redis
from datetime import datetime, timedelta
import jwt
from typing import List, Dict

class TokenService:
    def __init__(self):
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True
        )
        self.secret_key = os.getenv('JWT_SECRET_KEY')
        self.access_token_ttl = 15 * 60  # 15ë¶„
        self.refresh_token_ttl = 7 * 24 * 3600  # 7ì¼

    def generate_access_token(self, user_id: str, permissions: List[str]) -> str:
        """
        Access Token ìƒì„± (15ë¶„ ìœ íš¨)

        Args:
            user_id: ì‚¬ìš©ì ID
            permissions: ê¶Œí•œ ëª©ë¡

        Returns:
            JWT Access Token
        """
        payload = {
            'user_id': user_id,
            'permissions': permissions,
            'type': 'access',
            'exp': datetime.utcnow() + timedelta(seconds=self.access_token_ttl),
            'iat': datetime.utcnow()
        }

        return jwt.encode(payload, self.secret_key, algorithm='HS256')

    def generate_refresh_token(self, user_id: str) -> str:
        """
        Refresh Token ìƒì„± ë° Redis ì €ì¥ (7ì¼ ìœ íš¨)

        Args:
            user_id: ì‚¬ìš©ì ID

        Returns:
            JWT Refresh Token
        """
        token_id = self._generate_token_id()

        payload = {
            'user_id': user_id,
            'token_id': token_id,
            'type': 'refresh',
            'exp': datetime.utcnow() + timedelta(seconds=self.refresh_token_ttl),
            'iat': datetime.utcnow()
        }

        token = jwt.encode(payload, self.secret_key, algorithm='HS256')

        # Redisì— ì €ì¥ (Token Revocation ì§€ì›)
        redis_key = f"refresh_token:{user_id}:{token_id}"
        self.redis_client.setex(redis_key, self.refresh_token_ttl, token)

        return token

    def refresh_access_token(self, refresh_token: str) -> Dict[str, str]:
        """
        Refresh Tokenìœ¼ë¡œ ìƒˆ Access Token ë°œê¸‰

        Args:
            refresh_token: Refresh Token

        Returns:
            { "access_token": "...", "expires_in": 900 }

        Raises:
            InvalidTokenError: í† í°ì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë¬´íš¨í™”ë¨
        """
        try:
            # Refresh Token ê²€ì¦
            payload = jwt.decode(refresh_token, self.secret_key, algorithms=['HS256'])

            if payload.get('type') != 'refresh':
                raise InvalidTokenError("Not a refresh token")

            user_id = payload['user_id']
            token_id = payload['token_id']

            # Redisì—ì„œ í† í° ì¡´ì¬ í™•ì¸ (Revocation ì²´í¬)
            redis_key = f"refresh_token:{user_id}:{token_id}"
            stored_token = self.redis_client.get(redis_key)

            if not stored_token or stored_token != refresh_token:
                raise InvalidTokenError("Token revoked or invalid")

            # ìƒˆ Access Token ë°œê¸‰
            permissions = self._get_user_permissions(user_id)
            access_token = self.generate_access_token(user_id, permissions)

            return {
                'access_token': access_token,
                'expires_in': self.access_token_ttl
            }

        except jwt.ExpiredSignatureError:
            raise InvalidTokenError("Refresh token expired")
        except jwt.InvalidTokenError as e:
            raise InvalidTokenError(f"Invalid token: {str(e)}")

    def revoke_refresh_token(self, user_id: str, token_id: str = None):
        """
        Refresh Token ë¬´íš¨í™” (ë¡œê·¸ì•„ì›ƒ)

        Args:
            user_id: ì‚¬ìš©ì ID
            token_id: íŠ¹ì • í† í° ID (Noneì´ë©´ ëª¨ë“  í† í°)
        """
        if token_id:
            # íŠ¹ì • í† í°ë§Œ ë¬´íš¨í™”
            redis_key = f"refresh_token:{user_id}:{token_id}"
            self.redis_client.delete(redis_key)
        else:
            # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  Refresh Token ë¬´íš¨í™”
            pattern = f"refresh_token:{user_id}:*"
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)

    def revoke_all_tokens(self, user_id: str):
        """ëª¨ë“  í† í° ë¬´íš¨í™” (ë³´ì•ˆ ì´ë²¤íŠ¸ ì‹œ)"""
        self.revoke_refresh_token(user_id)

        # Access Token Blacklist ì¶”ê°€ (ë§Œë£Œë  ë•Œê¹Œì§€)
        # ì‹¤ì œ êµ¬í˜„ ì‹œ Access Tokenë„ Redis blacklistì— ì¶”ê°€

    def _generate_token_id(self) -> str:
        """ê³ ìœ  Token ID ìƒì„±"""
        import uuid
        return str(uuid.uuid4())

    def _get_user_permissions(self, user_id: str) -> List[str]:
        """ì‚¬ìš©ì ê¶Œí•œ ì¡°íšŒ (DBì—ì„œ)"""
        # ì‹¤ì œ êµ¬í˜„ ì‹œ DB ì¡°íšŒ
        return ['job:submit', 'node:view', 'data:read']


class InvalidTokenError(Exception):
    """í† í° ê²€ì¦ ì‹¤íŒ¨"""
    pass
```

**ìƒˆ API ì—”ë“œí¬ì¸íŠ¸:** `auth_portal_4430/auth_api.py`

```python
from flask import Blueprint, request, jsonify
from token_service import TokenService, InvalidTokenError

auth_bp = Blueprint('auth', __name__)
token_service = TokenService()

@auth_bp.route('/api/auth/login', methods=['POST'])
def login():
    """
    ë¡œê·¸ì¸ ë° í† í° ë°œê¸‰

    Request Body:
        { "username": "...", "password": "..." }

    Returns:
        {
            "access_token": "...",
            "refresh_token": "...",
            "expires_in": 900
        }
    """
    data = request.get_json()
    username = data.get('username')
    password = data.get('password')

    # ì‚¬ìš©ì ì¸ì¦ (ê¸°ì¡´ ë¡œì§)
    user = authenticate_user(username, password)

    if not user:
        return jsonify({'error': 'Invalid credentials'}), 401

    # í† í° ë°œê¸‰
    access_token = token_service.generate_access_token(
        user['id'],
        user['permissions']
    )
    refresh_token = token_service.generate_refresh_token(user['id'])

    return jsonify({
        'access_token': access_token,
        'refresh_token': refresh_token,
        'expires_in': 900,  # 15ë¶„
        'user': {
            'id': user['id'],
            'username': user['username'],
            'permissions': user['permissions']
        }
    }), 200


@auth_bp.route('/api/auth/refresh', methods=['POST'])
def refresh():
    """
    Access Token ê°±ì‹ 

    Request Body:
        { "refresh_token": "..." }

    Returns:
        {
            "access_token": "...",
            "expires_in": 900
        }
    """
    data = request.get_json()
    refresh_token = data.get('refresh_token')

    if not refresh_token:
        return jsonify({'error': 'Refresh token required'}), 400

    try:
        result = token_service.refresh_access_token(refresh_token)
        return jsonify(result), 200

    except InvalidTokenError as e:
        return jsonify({'error': str(e)}), 401


@auth_bp.route('/api/auth/logout', methods=['POST'])
def logout():
    """
    ë¡œê·¸ì•„ì›ƒ (Refresh Token ë¬´íš¨í™”)

    Headers:
        Authorization: Bearer <access_token>

    Request Body:
        { "refresh_token": "..." }  (ì„ íƒ)

    Returns:
        { "message": "Logged out successfully" }
    """
    # Access Tokenì—ì„œ user_id ì¶”ì¶œ
    auth_header = request.headers.get('Authorization')
    if not auth_header:
        return jsonify({'error': 'Authorization required'}), 401

    try:
        token = auth_header.split(' ')[1]
        payload = jwt.decode(token, token_service.secret_key, algorithms=['HS256'])
        user_id = payload['user_id']

        # Refresh Token ë¬´íš¨í™”
        data = request.get_json() or {}
        refresh_token = data.get('refresh_token')

        if refresh_token:
            # íŠ¹ì • í† í°ë§Œ ë¬´íš¨í™”
            refresh_payload = jwt.decode(refresh_token, token_service.secret_key, algorithms=['HS256'])
            token_id = refresh_payload.get('token_id')
            token_service.revoke_refresh_token(user_id, token_id)
        else:
            # ëª¨ë“  Refresh Token ë¬´íš¨í™”
            token_service.revoke_refresh_token(user_id)

        return jsonify({'message': 'Logged out successfully'}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 401
```

### 4.2 CORS Configuration (Production)

**ìˆ˜ì •:** `backend_5010/app.py`

```python
from flask_cors import CORS
import os

# í™˜ê²½ ë³€ìˆ˜ë¡œ CORS ì„¤ì • ì œì–´
FLASK_ENV = os.getenv('FLASK_ENV', 'development')
ALLOWED_ORIGINS = os.getenv('ALLOWED_ORIGINS', '').split(',')

if FLASK_ENV == 'development':
    # ê°œë°œ í™˜ê²½ - ëª¨ë“  origin í—ˆìš©
    CORS(app, origins='*', supports_credentials=True)
else:
    # í”„ë¡œë•ì…˜ - íŠ¹ì • originë§Œ í—ˆìš©
    if not ALLOWED_ORIGINS or ALLOWED_ORIGINS == ['']:
        raise ValueError("ALLOWED_ORIGINS must be set in production")

    CORS(app,
         origins=ALLOWED_ORIGINS,
         supports_credentials=True,
         allow_headers=['Content-Type', 'Authorization', 'X-Requested-With'],
         expose_headers=['X-Total-Count', 'X-Page-Count', 'Content-Disposition'],
         methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'PATCH'],
         max_age=3600)  # Preflight cache 1ì‹œê°„
```

**í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:** `.env.production`

```bash
FLASK_ENV=production
ALLOWED_ORIGINS=https://dashboard.example.com,https://cae.example.com,https://auth.example.com
JWT_SECRET_KEY=<ê°•ë ¥í•œ-ì‹œí¬ë¦¿-í‚¤>
REDIS_HOST=localhost
REDIS_PORT=6379
```

### 4.3 Rate Limiting

**ìƒˆ íŒŒì¼:** `backend_5010/middleware/rate_limiter.py`

```python
from flask import request, jsonify
from functools import wraps
import redis
import time

class RateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client

    def limit(self, max_requests: int, window: int):
        """
        Rate Limiting ë°ì½”ë ˆì´í„°

        Args:
            max_requests: ìµœëŒ€ ìš”ì²­ ìˆ˜
            window: ì‹œê°„ ìœˆë„ìš° (ì´ˆ)

        Example:
            @rate_limiter.limit(max_requests=100, window=60)  # ë¶„ë‹¹ 100íšŒ
            def my_endpoint():
                ...
        """
        def decorator(f):
            @wraps(f)
            def wrapped(*args, **kwargs):
                # í´ë¼ì´ì–¸íŠ¸ ì‹ë³„ (IP ë˜ëŠ” User ID)
                client_id = self._get_client_id()
                key = f"rate_limit:{f.__name__}:{client_id}"

                # í˜„ì¬ ìš”ì²­ ìˆ˜ í™•ì¸
                current = self.redis.get(key)

                if current and int(current) >= max_requests:
                    return jsonify({
                        'error': 'Rate limit exceeded',
                        'retry_after': self.redis.ttl(key)
                    }), 429

                # ì¹´ìš´í„° ì¦ê°€
                pipe = self.redis.pipeline()
                pipe.incr(key)
                pipe.expire(key, window)
                pipe.execute()

                return f(*args, **kwargs)

            return wrapped
        return decorator

    def _get_client_id(self) -> str:
        """í´ë¼ì´ì–¸íŠ¸ ì‹ë³„ì ì¶”ì¶œ"""
        # JWTì—ì„œ user_id ì¶”ì¶œ (ì¸ì¦ëœ ì‚¬ìš©ì)
        auth_header = request.headers.get('Authorization')
        if auth_header:
            try:
                token = auth_header.split(' ')[1]
                payload = jwt.decode(token, JWT_SECRET, algorithms=['HS256'])
                return f"user:{payload['user_id']}"
            except:
                pass

        # IP ì£¼ì†Œë¡œ fallback
        return f"ip:{request.remote_addr}"


# ì‚¬ìš© ì˜ˆì œ
redis_client = redis.Redis(host='localhost', port=6379, db=0)
rate_limiter = RateLimiter(redis_client)

@app.route('/api/jobs/submit', methods=['POST'])
@jwt_required
@rate_limiter.limit(max_requests=10, window=60)  # ë¶„ë‹¹ 10íšŒ
def submit_job():
    # Job ì œì¶œ ë¡œì§
    pass

@app.route('/api/apptainer/scan', methods=['POST'])
@jwt_required
@rate_limiter.limit(max_requests=5, window=300)  # 5ë¶„ë‹¹ 5íšŒ (ë¬´ê±°ìš´ ì‘ì—…)
def scan_apptainer():
    # ì´ë¯¸ì§€ ìŠ¤ìº” ë¡œì§
    pass
```

### 4.4 API Key Management

**ìƒˆ íŒŒì¼:** `backend_5010/api_key_service.py`

```python
import secrets
import hashlib
from datetime import datetime, timedelta
from typing import Optional, List

class APIKeyService:
    """
    ì™¸ë¶€ í”„ë¡ íŠ¸ì—”ë“œìš© API Key ê´€ë¦¬
    """

    def __init__(self, db_connection):
        self.db = db_connection

    def create_api_key(
        self,
        user_id: str,
        name: str,
        permissions: List[str],
        expires_in_days: int = 90
    ) -> dict:
        """
        API Key ìƒì„±

        Args:
            user_id: ì‚¬ìš©ì ID
            name: API Key ì´ë¦„ (ìš©ë„ ì‹ë³„)
            permissions: ê¶Œí•œ ëª©ë¡
            expires_in_days: ìœ íš¨ ê¸°ê°„ (ì¼)

        Returns:
            {
                "api_key": "sk_live_...",  # í•œ ë²ˆë§Œ í‘œì‹œ
                "key_id": "...",
                "created_at": "...",
                "expires_at": "..."
            }
        """
        # API Key ìƒì„± (32ë°”ì´íŠ¸ ëœë¤)
        api_key = f"sk_live_{secrets.token_urlsafe(32)}"

        # í•´ì‹œ ì €ì¥ (ì›ë³¸ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        key_hash = hashlib.sha256(api_key.encode()).hexdigest()

        # DB ì €ì¥
        cursor = self.db.cursor()
        expires_at = datetime.utcnow() + timedelta(days=expires_in_days)

        cursor.execute('''
            INSERT INTO api_keys (
                user_id, name, key_hash, permissions,
                created_at, expires_at, is_active
            ) VALUES (?, ?, ?, ?, datetime('now'), ?, 1)
        ''', (user_id, name, key_hash, ','.join(permissions), expires_at))

        key_id = cursor.lastrowid
        self.db.commit()

        return {
            'api_key': api_key,  # âš ï¸ í•œ ë²ˆë§Œ í‘œì‹œ, ë‹¤ì‹œ ì¡°íšŒ ë¶ˆê°€
            'key_id': key_id,
            'created_at': datetime.utcnow().isoformat(),
            'expires_at': expires_at.isoformat()
        }

    def validate_api_key(self, api_key: str) -> Optional[dict]:
        """
        API Key ê²€ì¦

        Args:
            api_key: API Key

        Returns:
            { "user_id": "...", "permissions": [...] } ë˜ëŠ” None
        """
        key_hash = hashlib.sha256(api_key.encode()).hexdigest()

        cursor = self.db.cursor()
        cursor.execute('''
            SELECT user_id, permissions, expires_at, is_active
            FROM api_keys
            WHERE key_hash = ?
        ''', (key_hash,))

        row = cursor.fetchone()

        if not row:
            return None

        # ë§Œë£Œ í™•ì¸
        expires_at = datetime.fromisoformat(row['expires_at'])
        if datetime.utcnow() > expires_at:
            return None

        # í™œì„±í™” í™•ì¸
        if not row['is_active']:
            return None

        # ë§ˆì§€ë§‰ ì‚¬ìš© ì‹œê°„ ì—…ë°ì´íŠ¸
        cursor.execute('''
            UPDATE api_keys
            SET last_used_at = datetime('now')
            WHERE key_hash = ?
        ''', (key_hash,))
        self.db.commit()

        return {
            'user_id': row['user_id'],
            'permissions': row['permissions'].split(',')
        }

    def revoke_api_key(self, key_id: int, user_id: str) -> bool:
        """API Key ë¬´íš¨í™”"""
        cursor = self.db.cursor()
        cursor.execute('''
            UPDATE api_keys
            SET is_active = 0, revoked_at = datetime('now')
            WHERE id = ? AND user_id = ?
        ''', (key_id, user_id))
        self.db.commit()
        return cursor.rowcount > 0

    def list_api_keys(self, user_id: str) -> List[dict]:
        """ì‚¬ìš©ìì˜ API Key ëª©ë¡ ì¡°íšŒ"""
        cursor = self.db.cursor()
        cursor.execute('''
            SELECT id, name, created_at, expires_at, last_used_at, is_active
            FROM api_keys
            WHERE user_id = ?
            ORDER BY created_at DESC
        ''', (user_id,))

        return [dict(row) for row in cursor.fetchall()]
```

**API ì—”ë“œí¬ì¸íŠ¸:** `backend_5010/api_key_api.py`

```python
from flask import Blueprint, request, jsonify

api_key_bp = Blueprint('api_keys', __name__)

@api_key_bp.route('/api/v2/api-keys', methods=['POST'])
@jwt_required
def create_api_key():
    """
    API Key ìƒì„±

    Request Body:
        {
            "name": "External CAE App",
            "permissions": ["job:submit", "job:view"],
            "expires_in_days": 90
        }
    """
    data = request.get_json()
    user_id = g.user_id

    result = api_key_service.create_api_key(
        user_id=user_id,
        name=data['name'],
        permissions=data['permissions'],
        expires_in_days=data.get('expires_in_days', 90)
    )

    return jsonify(result), 201


@api_key_bp.route('/api/v2/api-keys', methods=['GET'])
@jwt_required
def list_api_keys():
    """API Key ëª©ë¡ ì¡°íšŒ"""
    user_id = g.user_id
    keys = api_key_service.list_api_keys(user_id)
    return jsonify({'api_keys': keys}), 200


@api_key_bp.route('/api/v2/api-keys/<int:key_id>', methods=['DELETE'])
@jwt_required
def revoke_api_key(key_id: int):
    """API Key ë¬´íš¨í™”"""
    user_id = g.user_id
    success = api_key_service.revoke_api_key(key_id, user_id)

    if success:
        return jsonify({'message': 'API key revoked'}), 200
    else:
        return jsonify({'error': 'API key not found'}), 404
```

**Middleware:** API Key ì¸ì¦ ì§€ì›

```python
from functools import wraps

def api_key_required(f):
    """API Key ì¸ì¦ ë°ì½”ë ˆì´í„°"""
    @wraps(f)
    def decorated(*args, **kwargs):
        # Headerì—ì„œ API Key ì¶”ì¶œ
        api_key = request.headers.get('X-API-Key')

        if not api_key:
            return jsonify({'error': 'API Key required'}), 401

        # API Key ê²€ì¦
        auth_info = api_key_service.validate_api_key(api_key)

        if not auth_info:
            return jsonify({'error': 'Invalid or expired API Key'}), 401

        # Request contextì— ì‚¬ìš©ì ì •ë³´ ì €ì¥
        g.user_id = auth_info['user_id']
        g.permissions = auth_info['permissions']
        g.auth_method = 'api_key'

        return f(*args, **kwargs)

    return decorated


# ì‚¬ìš© ì˜ˆì œ
@app.route('/api/v2/upload/external', methods=['POST'])
@api_key_required
@permission_required(['job:submit'])
def external_upload():
    """ì™¸ë¶€ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ API Keyë¡œ ì ‘ê·¼"""
    # íŒŒì¼ ì—…ë¡œë“œ ë¡œì§
    pass
```

### 4.5 DB ë§ˆì´ê·¸ë ˆì´ì…˜

**ìƒˆ íŒŒì¼:** `backend_5010/migrations/v4.4.0_security.sql`

```sql
-- ============================================
-- v4.4.0: Security & Infrastructure
-- ì‘ì„±ì¼: 2025-11-05
-- ì„¤ëª…: API Key ê´€ë¦¬ ë° ë³´ì•ˆ ê°•í™”
-- ============================================

-- API Keys í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS api_keys (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    name TEXT NOT NULL,                    -- API Key ì´ë¦„
    key_hash TEXT NOT NULL UNIQUE,         -- SHA-256 í•´ì‹œ
    permissions TEXT NOT NULL,             -- ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê¶Œí•œ

    created_at TEXT NOT NULL,
    expires_at TEXT NOT NULL,
    last_used_at TEXT,
    revoked_at TEXT,

    is_active INTEGER DEFAULT 1,           -- 0: ë¹„í™œì„±, 1: í™œì„±

    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);

CREATE INDEX IF NOT EXISTS idx_api_keys_user_id ON api_keys(user_id);
CREATE INDEX IF NOT EXISTS idx_api_keys_key_hash ON api_keys(key_hash);
CREATE INDEX IF NOT EXISTS idx_api_keys_active ON api_keys(is_active, expires_at);

-- Audit Log í…Œì´ë¸” (ë³´ì•ˆ ì´ë²¤íŠ¸ ì¶”ì )
CREATE TABLE IF NOT EXISTS audit_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT,
    action TEXT NOT NULL,                  -- login, logout, api_call, etc.
    resource TEXT,                         -- ì ‘ê·¼í•œ ë¦¬ì†ŒìŠ¤
    method TEXT,                           -- HTTP method
    ip_address TEXT,
    user_agent TEXT,

    status_code INTEGER,                   -- HTTP status
    error_message TEXT,

    timestamp TEXT NOT NULL DEFAULT (datetime('now'))
);

CREATE INDEX IF NOT EXISTS idx_audit_logs_user_id ON audit_logs(user_id, timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_audit_logs_action ON audit_logs(action, timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_audit_logs_timestamp ON audit_logs(timestamp DESC);

-- Security Events í…Œì´ë¸” (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™)
CREATE TABLE IF NOT EXISTS security_events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT,
    event_type TEXT NOT NULL,              -- failed_login, rate_limit, invalid_token
    severity TEXT NOT NULL,                -- low, medium, high, critical
    description TEXT,
    ip_address TEXT,

    detected_at TEXT NOT NULL DEFAULT (datetime('now')),
    resolved_at TEXT,
    is_resolved INTEGER DEFAULT 0
);

CREATE INDEX IF NOT EXISTS idx_security_events_user_id ON security_events(user_id, detected_at DESC);
CREATE INDEX IF NOT EXISTS idx_security_events_severity ON security_events(severity, is_resolved);
```

### 4.6 Redis ì„¤ì¹˜ ë° ì„¤ì •

**Setup ìŠ¤í¬ë¦½íŠ¸:** `cluster/setup/phase4_security.sh`

```bash
#!/bin/bash

# ============================================
# Phase 4: Security & Infrastructure Setup
# ============================================

set -euo pipefail

log_info() { echo "[INFO] $1"; }
log_success() { echo "[SUCCESS] âœ… $1"; }
log_error() { echo "[ERROR] âŒ $1" >&2; }

log_info "Phase 4: Security & Infrastructure Setup"

# 1. Redis ì„¤ì¹˜
log_info "Installing Redis..."
sudo apt-get update
sudo apt-get install -y redis-server

# Redis ì„¤ì •
sudo systemctl enable redis-server
sudo systemctl start redis-server

# Redis ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
REDIS_PASSWORD=$(openssl rand -base64 32)
sudo sed -i "s/# requirepass foobared/requirepass $REDIS_PASSWORD/" /etc/redis/redis.conf
sudo systemctl restart redis-server

log_success "Redis installed and configured"

# 2. JWT Secret Key ìƒì„±
log_info "Generating JWT Secret Key..."
JWT_SECRET=$(openssl rand -base64 64)

# í™˜ê²½ ë³€ìˆ˜ ì €ì¥
cat > /home/koopark/web_services/backend/.env.security <<EOF
JWT_SECRET_KEY=$JWT_SECRET
JWT_ACCESS_TOKEN_TTL=900
JWT_REFRESH_TOKEN_TTL=604800
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=$REDIS_PASSWORD
FLASK_ENV=production
ALLOWED_ORIGINS=https://dashboard.example.com,https://cae.example.com
EOF

log_success "Security environment variables configured"

# 3. DB ë§ˆì´ê·¸ë ˆì´ì…˜
log_info "Running security migrations..."
cd /home/koopark/web_services/backend
python3 run_migrations.py

log_success "Security migrations completed"

# 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
log_info "Restarting services..."
sudo systemctl restart dashboard_backend
sudo systemctl restart auth_backend

log_success "Phase 4 completed!"
```

### 4.7 ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Redis ì„¤ì¹˜ ë° ì„¤ì •
- [ ] `token_service.py` êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] `auth_api.py` ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [ ] JWT Refresh Token í”Œë¡œìš° í…ŒìŠ¤íŠ¸
- [ ] Rate Limiting êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] API Key ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] CORS ì„¤ì • ì—…ë°ì´íŠ¸
- [ ] Audit Log ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] DB ë§ˆì´ê·¸ë ˆì´ì…˜ v4.4.0 ì‹¤í–‰
- [ ] ë³´ì•ˆ í…ŒìŠ¤íŠ¸ (ì¹¨íˆ¬ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
- [ ] Frontend ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ í”Œë¡œìš° ì—…ë°ì´íŠ¸
- [ ] ë¬¸ì„œí™” (API Key ì‚¬ìš© ê°€ì´ë“œ)

---

## âš¡ Phase 5: Performance Optimization (ìš°ì„ ìˆœìœ„ 3)

**ëª©í‘œ:** ë°±ì—”ë“œ/í”„ë¡ íŠ¸ì—”ë“œ ì„±ëŠ¥ ìµœì í™”
**ì˜ˆìƒ ê¸°ê°„:** 1.5ì£¼
**ì˜ì¡´ì„±:** ì—†ìŒ (ë…ë¦½ì  êµ¬í˜„ ê°€ëŠ¥)

### 5.1 Backend Async Processing

**ëª©ì :** SSH ê¸°ë°˜ ë…¸ë“œ ì¡°íšŒë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µ ì†ë„ í–¥ìƒ

**ìˆ˜ì •:** `backend_5010/slurm_utils_async.py`

```python
import asyncio
import asyncssh
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

class AsyncSlurmClient:
    """
    ë¹„ë™ê¸° Slurm í´ë¼ì´ì–¸íŠ¸

    SSH ì—°ê²°ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‹¤ìˆ˜ì˜ ë…¸ë“œ ì •ë³´ë¥¼
    ë¹ ë¥´ê²Œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """

    def __init__(self, ssh_username: str = 'slurm'):
        self.ssh_username = ssh_username
        self.connection_pool = {}

    async def get_node_info_async(self, nodes: List[str]) -> Dict[str, dict]:
        """
        ë³‘ë ¬ ë…¸ë“œ ì •ë³´ ì¡°íšŒ

        Args:
            nodes: ë…¸ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            { "node001": {...}, "node002": {...}, ... }
        """
        tasks = [self._query_node(node) for node in nodes]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        return {
            node: result if not isinstance(result, Exception) else None
            for node, result in zip(nodes, results)
        }

    async def _query_node(self, node: str) -> dict:
        """
        ë‹¨ì¼ ë…¸ë“œ ì •ë³´ ì¡°íšŒ

        Args:
            node: ë…¸ë“œ ì´ë¦„

        Returns:
            ë…¸ë“œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            async with asyncssh.connect(
                node,
                username=self.ssh_username,
                known_hosts=None,
                client_keys=['/home/slurm/.ssh/id_rsa']
            ) as conn:
                # sinfo ì‹¤í–‰
                result = await conn.run(
                    f'sinfo -n {node} -o "%C,%m,%t,%O,%E"',
                    check=True
                )

                return self._parse_sinfo(result.stdout)

        except asyncssh.Error as e:
            logger.error(f"SSH connection failed for {node}: {e}")
            return None
        except Exception as e:
            logger.error(f"Error querying node {node}: {e}")
            return None

    def _parse_sinfo(self, output: str) -> dict:
        """sinfo ì¶œë ¥ íŒŒì‹±"""
        # CPUs, Memory, State, CPUsLoad, Reason íŒŒì‹±
        parts = output.strip().split(',')

        if len(parts) < 3:
            return {}

        cpus = parts[0]  # "4/0/0/4" (allocated/idle/other/total)
        memory = parts[1]
        state = parts[2]

        return {
            'cpus': cpus,
            'memory': int(memory) if memory.isdigit() else 0,
            'state': state,
            'load': parts[3] if len(parts) > 3 else 'N/A',
            'reason': parts[4] if len(parts) > 4 else ''
        }

    async def submit_multiple_jobs(self, jobs: List[dict]) -> List[str]:
        """
        ë³‘ë ¬ ì‘ì—… ì œì¶œ

        Args:
            jobs: Job ì„¤ì • ë¦¬ìŠ¤íŠ¸

        Returns:
            Job ID ë¦¬ìŠ¤íŠ¸
        """
        tasks = [self._submit_job(job) for job in jobs]
        job_ids = await asyncio.gather(*tasks, return_exceptions=True)

        return [
            job_id if not isinstance(job_id, Exception) else None
            for job_id in job_ids
        ]

    async def _submit_job(self, job: dict) -> str:
        """
        ë‹¨ì¼ ì‘ì—… ì œì¶œ

        Args:
            job: Job ì„¤ì •

        Returns:
            Job ID
        """
        try:
            script = self._generate_job_script(job)

            async with asyncssh.connect(
                'slurmctld',
                username=self.ssh_username,
                known_hosts=None
            ) as conn:
                result = await conn.run('sbatch', input=script, check=True)
                return self._extract_job_id(result.stdout)

        except Exception as e:
            logger.error(f"Job submission failed: {e}")
            raise

    def _generate_job_script(self, job: dict) -> str:
        """Job ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        return f"""#!/bin/bash
#SBATCH --job-name={job['name']}
#SBATCH --partition={job['partition']}
#SBATCH --nodes={job['nodes']}
#SBATCH --ntasks={job['tasks']}
#SBATCH --time={job['time']}

{job['command']}
"""

    def _extract_job_id(self, output: str) -> str:
        """sbatch ì¶œë ¥ì—ì„œ Job ID ì¶”ì¶œ"""
        # "Submitted batch job 12345" -> "12345"
        import re
        match = re.search(r'Submitted batch job (\d+)', output)
        return match.group(1) if match else None


# Flaskì—ì„œ ì‚¬ìš©
async_slurm = AsyncSlurmClient()

@app.route('/api/nodes/status', methods=['GET'])
async def get_nodes_status():
    """
    ëª¨ë“  ë…¸ë“œ ìƒíƒœ ì¡°íšŒ (ë¹„ë™ê¸°)

    ê¸°ì¡´ ë™ê¸° ë°©ì‹ (4 nodes Ã— 2s = 8s)
    â†’ ë¹„ë™ê¸° ë°©ì‹ (max 2s)
    """
    nodes = ['node001', 'node002', 'viz-node001', 'viz-node002']

    results = await async_slurm.get_node_info_async(nodes)

    return jsonify({
        'nodes': results,
        'total': len(nodes),
        'online': sum(1 for v in results.values() if v and v.get('state') != 'down')
    }), 200
```

### 5.2 Database Query Optimization

**ëª©ì :** ìì£¼ ì¡°íšŒë˜ëŠ” ì¿¼ë¦¬ ìµœì í™” ë° ì¸ë±ìŠ¤ ì¶”ê°€

**ìƒˆ ë§ˆì´ê·¸ë ˆì´ì…˜:** `backend_5010/migrations/v4.5.0_performance.sql`

```sql
-- ============================================
-- v4.5.0: Performance Optimization
-- ì‘ì„±ì¼: 2025-11-05
-- ì„¤ëª…: DB ì¸ë±ìŠ¤ ìµœì í™” ë° ì¿¼ë¦¬ ì„±ëŠ¥ ê°œì„ 
-- ============================================

-- ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€

-- ì•Œë¦¼ ì¡°íšŒ ìµœì í™” (ì‚¬ìš©ìë³„ ë¯¸ì½ì€ ì•Œë¦¼)
CREATE INDEX IF NOT EXISTS idx_notifications_user_read_time
ON notifications(created_by, read, timestamp DESC);

-- í…œí”Œë¦¿ ì¡°íšŒ ìµœì í™” (ì¹´í…Œê³ ë¦¬ë³„ ê³µê°œ í…œí”Œë¦¿)
CREATE INDEX IF NOT EXISTS idx_templates_category_public
ON job_templates_v2(category, is_public, created_at DESC);

-- Job íˆìŠ¤í† ë¦¬ ì¡°íšŒ ìµœì í™” (ì‚¬ìš©ìë³„ ìƒíƒœë³„ ì •ë ¬)
CREATE INDEX IF NOT EXISTS idx_jobs_user_status_time
ON job_history(user_id, status, submit_time DESC);

-- ì—…ë¡œë“œ ì„¸ì…˜ ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_uploads_user_status_time
ON file_uploads(user_id, status, created_at DESC);

-- Apptainer ì´ë¯¸ì§€ ì¡°íšŒ ìµœì í™” (íŒŒí‹°ì…˜ë³„)
CREATE INDEX IF NOT EXISTS idx_apptainer_partition_type
ON apptainer_images(partition, type, name);

-- Audit Log ì¡°íšŒ ìµœì í™”
CREATE INDEX IF NOT EXISTS idx_audit_user_time
ON audit_logs(user_id, timestamp DESC);

CREATE INDEX IF NOT EXISTS idx_audit_action_time
ON audit_logs(action, timestamp DESC);

-- ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ (ì‹¤í–‰ ê³„íš í™•ì¸)
-- EXPLAIN QUERY PLAN SELECT * FROM notifications WHERE created_by = 'user123' AND read = 0 ORDER BY timestamp DESC LIMIT 20;

-- í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
ANALYZE;
```

**ì¿¼ë¦¬ ìµœì í™” ì˜ˆì œ:**

```python
# âŒ ë¹„íš¨ìœ¨ì  ì¿¼ë¦¬ (N+1 ë¬¸ì œ)
def get_jobs_with_templates():
    jobs = db.execute("SELECT * FROM job_history").fetchall()
    for job in jobs:
        template = db.execute(
            "SELECT * FROM job_templates_v2 WHERE template_id = ?",
            (job['template_id'],)
        ).fetchone()
        job['template'] = template
    return jobs

# âœ… ìµœì í™”ëœ ì¿¼ë¦¬ (JOIN ì‚¬ìš©)
def get_jobs_with_templates_optimized():
    return db.execute("""
        SELECT
            j.*,
            t.name as template_name,
            t.category as template_category
        FROM job_history j
        LEFT JOIN job_templates_v2 t ON j.template_id = t.template_id
        ORDER BY j.submit_time DESC
        LIMIT 50
    """).fetchall()
```

### 5.3 Frontend Code Splitting

**ëª©ì :** ë²ˆë“¤ í¬ê¸° ìµœì í™”ë¡œ ì´ˆê¸° ë¡œë”© ì†ë„ ê°œì„ 

**ìˆ˜ì •:** `frontend_3010/vite.config.ts`

```typescript
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';
import { visualizer } from 'rollup-plugin-visualizer';

export default defineConfig({
  plugins: [
    react(),
    visualizer({
      filename: './dist/stats.html',
      open: false,
      gzipSize: true,
      brotliSize: true
    })
  ],
  build: {
    rollupOptions: {
      output: {
        manualChunks: {
          // Vendor chunks (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶„ë¦¬)
          'react-vendor': ['react', 'react-dom', 'react-router-dom'],
          'ui-vendor': ['lucide-react', '@heroicons/react', 'clsx'],
          'state-vendor': ['zustand'],
          'chart-vendor': ['recharts'],
          '3d-vendor': ['three', '@react-three/fiber', '@react-three/drei'],

          // Feature chunks (í˜ì´ì§€ë³„ ë¶„ë¦¬)
          'job-management': [
            './src/pages/JobManagement.tsx',
            './src/components/JobSubmit',
            './src/components/JobTemplates'
          ],
          'node-management': [
            './src/pages/NodeManagement.tsx',
            './src/components/NodeStatus'
          ],
          'ssh-vnc': [
            './src/components/SSHSessionManager.tsx',
            './src/components/VNCSessionManager.tsx'
          ],
          'data-management': [
            './src/pages/DataManagement.tsx',
            './src/components/FileUpload'
          ],
          'monitoring': [
            './src/pages/Dashboard.tsx',
            './src/components/Monitoring'
          ]
        }
      }
    },
    chunkSizeWarningLimit: 1000,  // 1MB
    minify: 'esbuild',
    target: 'es2020',
    sourcemap: false  // Productionì—ì„œëŠ” sourcemap ë¹„í™œì„±í™”
  },
  optimizeDeps: {
    include: ['react', 'react-dom', 'react-router-dom']
  }
});
```

**Lazy Loading ì ìš©:**

```typescript
// âŒ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í•œ ë²ˆì— ë¡œë“œ
import JobManagement from './pages/JobManagement';
import NodeManagement from './pages/NodeManagement';
import DataManagement from './pages/DataManagement';

// âœ… í•„ìš”í•  ë•Œë§Œ ë¡œë“œ (Lazy Loading)
import { lazy, Suspense } from 'react';

const JobManagement = lazy(() => import('./pages/JobManagement'));
const NodeManagement = lazy(() => import('./pages/NodeManagement'));
const DataManagement = lazy(() => import('./pages/DataManagement'));
const SSHManager = lazy(() => import('./components/SSHSessionManager'));
const VNCManager = lazy(() => import('./components/VNCSessionManager'));

function App() {
  return (
    <Suspense fallback={<LoadingSpinner />}>
      <Routes>
        <Route path="/jobs" element={<JobManagement />} />
        <Route path="/nodes" element={<NodeManagement />} />
        <Route path="/data" element={<DataManagement />} />
        <Route path="/ssh" element={<SSHManager />} />
        <Route path="/vnc" element={<VNCManager />} />
      </Routes>
    </Suspense>
  );
}
```

### 5.4 Caching Strategy

**Backend ìºì‹±:**

```python
from functools import lru_cache
import time

class CacheManager:
    def __init__(self):
        self.cache = {}
        self.ttl = {}

    def get(self, key: str) -> Optional[any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if key not in self.cache:
            return None

        # TTL í™•ì¸
        if time.time() > self.ttl.get(key, 0):
            del self.cache[key]
            del self.ttl[key]
            return None

        return self.cache[key]

    def set(self, key: str, value: any, ttl: int = 60):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        self.cache[key] = value
        self.ttl[key] = time.time() + ttl

    def delete(self, key: str):
        """ìºì‹œ ì‚­ì œ"""
        self.cache.pop(key, None)
        self.ttl.pop(key, None)

cache = CacheManager()

@app.route('/api/nodes/status', methods=['GET'])
def get_nodes_status_cached():
    """ë…¸ë“œ ìƒíƒœ ì¡°íšŒ (30ì´ˆ ìºì‹±)"""
    cache_key = 'nodes:status'

    # ìºì‹œ í™•ì¸
    cached_data = cache.get(cache_key)
    if cached_data:
        return jsonify(cached_data), 200

    # ë°ì´í„° ì¡°íšŒ
    nodes = get_all_nodes_status()

    # ìºì‹œ ì €ì¥ (30ì´ˆ)
    cache.set(cache_key, nodes, ttl=30)

    return jsonify(nodes), 200
```

### 5.5 WebSocket Connection Pooling

**ìˆ˜ì •:** `websocket_5011/websocket_server.py`

```python
# ì—°ê²° í’€ ê´€ë¦¬
class ConnectionPool:
    def __init__(self, max_connections: int = 1000):
        self.connections: Dict[str, web.WebSocketResponse] = {}
        self.max_connections = max_connections
        self.topics: Dict[str, Set[str]] = {}  # topic -> client_ids

    def add(self, client_id: str, ws: web.WebSocketResponse):
        """í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€"""
        if len(self.connections) >= self.max_connections:
            raise ConnectionError("Max connections reached")

        self.connections[client_id] = ws

    def remove(self, client_id: str):
        """í´ë¼ì´ì–¸íŠ¸ ì œê±°"""
        self.connections.pop(client_id, None)

        # ëª¨ë“  topicì—ì„œ ì œê±°
        for topic_clients in self.topics.values():
            topic_clients.discard(client_id)

    def subscribe(self, client_id: str, topic: str):
        """Topic êµ¬ë…"""
        if topic not in self.topics:
            self.topics[topic] = set()

        self.topics[topic].add(client_id)

    def unsubscribe(self, client_id: str, topic: str):
        """Topic êµ¬ë… í•´ì œ"""
        if topic in self.topics:
            self.topics[topic].discard(client_id)

    async def broadcast_to_topic(self, topic: str, message: dict):
        """íŠ¹ì • topic êµ¬ë…ìì—ê²Œë§Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if topic not in self.topics:
            return

        client_ids = self.topics[topic].copy()

        for client_id in client_ids:
            ws = self.connections.get(client_id)
            if ws:
                try:
                    await ws.send_json(message)
                except Exception:
                    self.remove(client_id)


pool = ConnectionPool()

async def websocket_handler(request):
    """WebSocket ì—°ê²° í•¸ë“¤ëŸ¬"""
    ws = web.WebSocketResponse()
    await ws.prepare(request)

    client_id = str(uuid.uuid4())
    pool.add(client_id, ws)

    try:
        async for msg in ws:
            if msg.type == web.WSMsgType.TEXT:
                data = json.loads(msg.data)

                # Topic êµ¬ë…
                if data.get('action') == 'subscribe':
                    pool.subscribe(client_id, data['topic'])

                # Topic êµ¬ë… í•´ì œ
                elif data.get('action') == 'unsubscribe':
                    pool.unsubscribe(client_id, data['topic'])
    finally:
        pool.remove(client_id)

    return ws


# ì‚¬ìš© ì˜ˆì œ: ì—…ë¡œë“œ ì§„í–‰ë¥ ì„ upload topic êµ¬ë…ìì—ê²Œë§Œ ì „ì†¡
async def broadcast_upload_progress(upload_id: str, progress: int):
    await pool.broadcast_to_topic('upload', {
        'type': 'upload_progress',
        'upload_id': upload_id,
        'progress': progress
    })
```

### 5.6 ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Backend ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬í˜„ (`slurm_utils_async.py`)
- [ ] DB ì¸ë±ìŠ¤ ìµœì í™” (v4.5.0 ë§ˆì´ê·¸ë ˆì´ì…˜)
- [ ] ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ë° ê°œì„ 
- [ ] Frontend Code Splitting ì ìš©
- [ ] Lazy Loading êµ¬í˜„
- [ ] ìºì‹± ì „ëµ êµ¬í˜„ (Backend/Frontend)
- [ ] WebSocket Connection Pooling
- [ ] ë²ˆë“¤ í¬ê¸° ë¶„ì„ (`stats.html` í™•ì¸)
- [ ] Lighthouse ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ëª©í‘œ: 90+ ì )
- [ ] Load Testing (ëª©í‘œ: 100 concurrent users)
- [ ] ë°°í¬ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

---

## ğŸ§ª Phase 6: Testing & Documentation (ìµœì¢… ë‹¨ê³„)

**ëª©í‘œ:** í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„± ë° ë¬¸ì„œí™”
**ì˜ˆìƒ ê¸°ê°„:** 1ì£¼
**ì˜ì¡´ì„±:** Phase 1-5 ì™„ë£Œ í›„

### 6.1 Backend Unit Tests

**ìƒˆ ë””ë ‰í† ë¦¬:** `backend_5010/tests/`

```bash
backend_5010/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # pytest fixtures
â”œâ”€â”€ test_apptainer_service.py
â”œâ”€â”€ test_template_manager.py
â”œâ”€â”€ test_file_upload.py
â”œâ”€â”€ test_auth_service.py
â”œâ”€â”€ test_api_keys.py
â””â”€â”€ test_async_slurm.py
```

**conftest.py - Test Fixtures**

```python
import pytest
import sqlite3
import tempfile
import os

@pytest.fixture
def test_db():
    """í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ DB"""
    fd, path = tempfile.mkstemp(suffix='.db')
    conn = sqlite3.connect(path)
    conn.row_factory = sqlite3.Row

    # í…Œì´ë¸” ìƒì„± (ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš©)
    with open('migrations/v4.1.0_apptainer_images.sql') as f:
        conn.executescript(f.read())

    yield conn

    conn.close()
    os.close(fd)
    os.unlink(path)


@pytest.fixture
def test_user():
    """í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì"""
    return {
        'id': 'test_user_001',
        'username': 'testuser',
        'permissions': ['job:submit', 'job:view']
    }
```

**test_file_upload.py**

```python
import pytest
from file_upload_api import file_upload_bp
from file_classifier import get_file_classifier

def test_file_classification():
    """íŒŒì¼ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
    classifier = get_file_classifier()

    # Data íŒŒì¼
    info = classifier.classify_file('dataset.tar.gz')
    assert info['type'] == 'data'
    assert info['is_compressed'] == True

    # Config íŒŒì¼
    info = classifier.classify_file('config.yaml')
    assert info['type'] == 'config'
    assert info['is_binary'] == False

    # Model íŒŒì¼
    info = classifier.classify_file('model_weights.pth')
    assert info['type'] == 'model'


def test_upload_init(client, test_user):
    """ì—…ë¡œë“œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    response = client.post('/api/v2/files/upload/init', json={
        'filename': 'test.dat',
        'file_size': 1024000,
        'user_id': test_user['id']
    })

    assert response.status_code == 201
    data = response.get_json()
    assert 'upload_id' in data
    assert 'chunk_size' in data
    assert data['total_chunks'] > 0


def test_upload_large_file(client, test_user):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    # 10GB íŒŒì¼
    file_size = 10 * 1024 * 1024 * 1024

    response = client.post('/api/v2/files/upload/init', json={
        'filename': 'large_dataset.tar.gz',
        'file_size': file_size,
        'user_id': test_user['id']
    })

    assert response.status_code == 201
    data = response.get_json()

    # 5MB ì²­í¬ = 2000 ì²­í¬
    assert data['total_chunks'] == 2000
    assert data['chunk_size'] == 5 * 1024 * 1024
```

### 6.2 Frontend Tests

**ìƒˆ ë””ë ‰í† ë¦¬:** `frontend_3010/src/__tests__/`

```bash
frontend_3010/src/__tests__/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ApptainerSelector.test.tsx
â”‚   â”œâ”€â”€ UnifiedUploader.test.tsx
â”‚   â””â”€â”€ FileClassifier.test.tsx
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useApptainerImages.test.ts
â”‚   â””â”€â”€ useFileUpload.test.ts
â””â”€â”€ utils/
    â””â”€â”€ ChunkUploader.test.ts
```

**UnifiedUploader.test.tsx**

```typescript
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { UnifiedUploader } from '../components/FileUpload/UnifiedUploader';

describe('UnifiedUploader', () => {
  it('renders upload interface', () => {
    render(
      <UnifiedUploader
        userId="test_user"
        onComplete={jest.fn()}
      />
    );

    expect(screen.getByText(/drag.*drop/i)).toBeInTheDocument();
    expect(screen.getByRole('button', { name: /select files/i })).toBeInTheDocument();
  });

  it('accepts file drag and drop', async () => {
    const onComplete = jest.fn();

    render(
      <UnifiedUploader
        userId="test_user"
        onComplete={onComplete}
      />
    );

    const dropzone = screen.getByTestId('dropzone');
    const file = new File(['test'], 'test.dat', { type: 'application/octet-stream' });

    fireEvent.drop(dropzone, {
      dataTransfer: {
        files: [file]
      }
    });

    await waitFor(() => {
      expect(screen.getByText('test.dat')).toBeInTheDocument();
    });
  });

  it('shows file classification', async () => {
    render(
      <UnifiedUploader
        userId="test_user"
        onComplete={jest.fn()}
      />
    );

    const files = [
      new File(['data'], 'input.dat'),
      new File(['config'], 'config.yaml'),
      new File(['script'], 'run.sh')
    ];

    // íŒŒì¼ ì¶”ê°€
    const input = screen.getByLabelText(/select files/i);
    fireEvent.change(input, { target: { files } });

    await waitFor(() => {
      expect(screen.getByText(/DATA Files \(1\)/i)).toBeInTheDocument();
      expect(screen.getByText(/CONFIG Files \(1\)/i)).toBeInTheDocument();
      expect(screen.getByText(/SCRIPT Files \(1\)/i)).toBeInTheDocument();
    });
  });

  it('shows upload progress', async () => {
    render(
      <UnifiedUploader
        userId="test_user"
        onComplete={jest.fn()}
      />
    );

    // íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘
    const file = new File(['x'.repeat(10000000)], 'large.dat');
    const input = screen.getByLabelText(/select files/i);
    fireEvent.change(input, { target: { files: [file] } });

    const uploadButton = screen.getByRole('button', { name: /start upload/i });
    fireEvent.click(uploadButton);

    await waitFor(() => {
      expect(screen.getByRole('progressbar')).toBeInTheDocument();
    });
  });
});
```

### 6.3 E2E Tests (Playwright)

**ìƒˆ ë””ë ‰í† ë¦¬:** `tests/e2e/`

```bash
tests/e2e/
â”œâ”€â”€ playwright.config.ts
â”œâ”€â”€ auth.setup.ts
â”œâ”€â”€ job-submit-flow.spec.ts
â”œâ”€â”€ file-upload-flow.spec.ts
â””â”€â”€ template-management.spec.ts
```

**job-submit-flow.spec.ts**

```typescript
import { test, expect } from '@playwright/test';

test.describe('Job Submit Flow', () => {
  test.beforeEach(async ({ page }) => {
    // ë¡œê·¸ì¸
    await page.goto('http://localhost:3010/login');
    await page.fill('input[name="username"]', 'testuser');
    await page.fill('input[name="password"]', 'testpass');
    await page.click('button[type="submit"]');

    await expect(page).toHaveURL('http://localhost:3010/dashboard');
  });

  test('complete job submission with file upload', async ({ page }) => {
    // 1. Job Management í˜ì´ì§€ë¡œ ì´ë™
    await page.click('a[href="/jobs"]');
    await expect(page).toHaveURL('http://localhost:3010/jobs');

    // 2. ìƒˆ Job ìƒì„±
    await page.click('button:has-text("New Job")');

    // 3. í…œí”Œë¦¿ ì„ íƒ
    await page.click('text=PyTorch Training');
    await page.click('button:has-text("Next")');

    // 4. íŒŒì¼ ì—…ë¡œë“œ
    await page.setInputFiles('input[type="file"]', [
      'tests/fixtures/training_data.tar.gz',
      'tests/fixtures/config.yaml'
    ]);

    // íŒŒì¼ ë¶„ë¥˜ í™•ì¸
    await expect(page.locator('text=DATA Files (1)')).toBeVisible();
    await expect(page.locator('text=CONFIG Files (1)')).toBeVisible();

    // ì—…ë¡œë“œ ì‹œì‘
    await page.click('button:has-text("Upload")');

    // ì§„í–‰ë¥  í™•ì¸
    await expect(page.locator('role=progressbar')).toBeVisible();

    // ì™„ë£Œ ëŒ€ê¸°
    await expect(page.locator('text=Upload completed')).toBeVisible({
      timeout: 60000
    });

    // 5. Job ì„¤ì •
    await page.fill('input[name="job_name"]', 'Test Training Job');
    await page.selectOption('select[name="partition"]', 'compute');
    await page.fill('input[name="nodes"]', '2');

    // 6. Job ì œì¶œ
    await page.click('button:has-text("Submit Job")');

    // 7. ì œì¶œ í™•ì¸
    await expect(page.locator('text=Job submitted successfully')).toBeVisible();
    await expect(page).toHaveURL(/\/jobs\/\d+/);
  });

  test('handles upload errors gracefully', async ({ page }) => {
    await page.goto('http://localhost:3010/jobs/new');

    // ë„ˆë¬´ í° íŒŒì¼ ì—…ë¡œë“œ ì‹œë„ (60GB)
    await page.evaluate(() => {
      const input = document.querySelector('input[type="file"]');
      const file = new File(['x'.repeat(60 * 1024 * 1024 * 1024)], 'huge.dat');

      const dataTransfer = new DataTransfer();
      dataTransfer.items.add(file);
      input.files = dataTransfer.files;
    });

    // ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
    await expect(page.locator('text=File size exceeds maximum')).toBeVisible();
  });
});
```

### 6.4 API Documentation (OpenAPI/Swagger)

**ìƒˆ íŒŒì¼:** `backend_5010/openapi.yaml`

```yaml
openapi: 3.0.0
info:
  title: Slurm Cluster Dashboard API
  version: 5.0.0
  description: |
    Dashboard API for Slurm cluster management

    ## Features
    - Apptainer image discovery
    - Template management
    - File upload with chunking
    - Job submission and monitoring
    - Authentication with JWT
  contact:
    name: Cluster Admin
    email: admin@example.com

servers:
  - url: http://localhost:5010
    description: Development
  - url: https://api.cluster.example.com
    description: Production

components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
    apiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key

  schemas:
    FileUploadInit:
      type: object
      required:
        - filename
        - file_size
        - user_id
      properties:
        filename:
          type: string
          example: "dataset.tar.gz"
        file_size:
          type: integer
          format: int64
          example: 10485760
        user_id:
          type: string
          example: "user_123"
        job_id:
          type: string
          nullable: true
          example: "job_456"
        chunk_size:
          type: integer
          default: 5242880
          example: 5242880

    FileUploadSession:
      type: object
      properties:
        upload_id:
          type: string
          example: "1652cdf37e4feb12"
        chunk_size:
          type: integer
          example: 5242880
        total_chunks:
          type: integer
          example: 2
        storage_path:
          type: string
          example: "/shared/uploads/jobs/job_456/data"
        file_info:
          $ref: '#/components/schemas/FileInfo'

    FileInfo:
      type: object
      properties:
        type:
          type: string
          enum: [data, config, script, model, mesh, result, document]
          example: "data"
        extension:
          type: string
          example: ".tar.gz"
        mime_type:
          type: string
          example: "application/x-tar"
        size:
          type: integer
          format: int64
          example: 10485760
        is_binary:
          type: boolean
          example: true
        is_compressed:
          type: boolean
          example: true

paths:
  /api/v2/files/upload/init:
    post:
      summary: Initialize file upload session
      tags:
        - File Upload
      security:
        - bearerAuth: []
        - apiKeyAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/FileUploadInit'
      responses:
        '201':
          description: Upload session created
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/FileUploadSession'
        '400':
          description: Bad request
        '401':
          description: Unauthorized

  /api/v2/files/upload/chunk:
    post:
      summary: Upload file chunk
      tags:
        - File Upload
      security:
        - bearerAuth: []
        - apiKeyAuth: []
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required:
                - upload_id
                - chunk_index
                - chunk
              properties:
                upload_id:
                  type: string
                chunk_index:
                  type: integer
                chunk:
                  type: string
                  format: binary
                checksum:
                  type: string
                  description: MD5 checksum (optional)
      responses:
        '200':
          description: Chunk uploaded
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
                  uploaded_chunks:
                    type: integer
                  total_chunks:
                    type: integer
                  progress:
                    type: number
                    format: float

  /api/v2/files/upload/complete:
    post:
      summary: Complete file upload
      tags:
        - File Upload
      security:
        - bearerAuth: []
        - apiKeyAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - upload_id
              properties:
                upload_id:
                  type: string
                verify_checksum:
                  type: boolean
                  default: false
      responses:
        '200':
          description: Upload completed
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
                  file_path:
                    type: string
                  file_info:
                    $ref: '#/components/schemas/FileInfo'
```

**Swagger UI ì„¤ì •:**

```python
from flask_swagger_ui import get_swaggerui_blueprint

SWAGGER_URL = '/api/docs'
API_URL = '/static/openapi.yaml'

swaggerui_blueprint = get_swaggerui_blueprint(
    SWAGGER_URL,
    API_URL,
    config={
        'app_name': "Slurm Dashboard API"
    }
)

app.register_blueprint(swaggerui_blueprint, url_prefix=SWAGGER_URL)
```

### 6.5 User Documentation

**ìƒˆ íŒŒì¼:** `docs/USER_GUIDE.md`

```markdown
# Slurm Dashboard User Guide

## ëª©ì°¨
1. [ì‹œì‘í•˜ê¸°](#ì‹œì‘í•˜ê¸°)
2. [Job ì œì¶œ](#job-ì œì¶œ)
3. [íŒŒì¼ ì—…ë¡œë“œ](#íŒŒì¼-ì—…ë¡œë“œ)
4. [í…œí”Œë¦¿ ê´€ë¦¬](#í…œí”Œë¦¿-ê´€ë¦¬)
5. [API ì‚¬ìš©ë²•](#api-ì‚¬ìš©ë²•)

## ì‹œì‘í•˜ê¸°

### ë¡œê·¸ì¸
1. ë¸Œë¼ìš°ì €ì—ì„œ `http://dashboard.example.com` ì ‘ì†
2. ì‚¬ìš©ìëª…ê³¼ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
3. ë¡œê·¸ì¸ ì„±ê³µ í›„ ëŒ€ì‹œë³´ë“œë¡œ ì´ë™

### ëŒ€ì‹œë³´ë“œ ê°œìš”
- **ì™¼ìª½ ë©”ë‰´**: ì£¼ìš” ê¸°ëŠ¥ íƒìƒ‰
- **ìƒë‹¨ ë°”**: ì•Œë¦¼, í”„ë¡œí•„ ì„¤ì •
- **ë©”ì¸ ì˜ì—­**: í˜„ì¬ í˜ì´ì§€ ì»¨í…ì¸ 

## Job ì œì¶œ

### ê¸°ë³¸ Job ì œì¶œ
1. ì™¼ìª½ ë©”ë‰´ì—ì„œ **"Job Management"** í´ë¦­
2. **"New Job"** ë²„íŠ¼ í´ë¦­
3. í…œí”Œë¦¿ ì„ íƒ ë˜ëŠ” **"Custom Job"** ì„ íƒ
4. í•„ìš”í•œ íŒŒì¼ ì—…ë¡œë“œ
5. Job ì„¤ì • ì…ë ¥:
   - Job Name
   - Partition (compute/viz)
   - Nodes
   - Tasks per node
   - Time limit
6. **"Submit Job"** í´ë¦­

### í…œí”Œë¦¿ ì‚¬ìš©
í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ë©´ ë¯¸ë¦¬ ì •ì˜ëœ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥´ê²Œ Jobì„ ì œì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. Job ìƒì„± ì‹œ **"Use Template"** ì„ íƒ
2. ì¹´í…Œê³ ë¦¬ë³„ í…œí”Œë¦¿ íƒìƒ‰:
   - ML (Machine Learning)
   - CFD (Computational Fluid Dynamics)
   - Structural Analysis
3. í…œí”Œë¦¿ ì„ íƒ í›„ **"Next"** í´ë¦­
4. í•„ìš”í•œ íŒŒì¼ ì—…ë¡œë“œ (í…œí”Œë¦¿ì´ ìš”êµ¬í•˜ëŠ” íŒŒì¼ íƒ€ì… í™•ì¸)
5. Job ì„¤ì • í™•ì¸ ë° ìˆ˜ì •
6. **"Submit"** í´ë¦­

## íŒŒì¼ ì—…ë¡œë“œ

### ì§€ì› íŒŒì¼ íƒ€ì…
- **Data**: .dat, .csv, .tar.gz, .hdf5, etc.
- **Config**: .yaml, .json, .toml, .ini
- **Script**: .py, .sh, .sbatch
- **Model**: .pth, .ckpt, .h5
- **Mesh**: .msh, .stl, .vtk

### ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ
1. **"Select Files"** ë²„íŠ¼ í´ë¦­ ë˜ëŠ” Drag & Drop
2. íŒŒì¼ ì„ íƒ
3. ìë™ìœ¼ë¡œ íŒŒì¼ íƒ€ì… ë¶„ë¥˜ë¨
4. **"Start Upload"** í´ë¦­

### ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ
- ìµœëŒ€ íŒŒì¼ í¬ê¸°: **50GB**
- ìë™ ì²­í¬ ì—…ë¡œë“œ (5MB ë‹¨ìœ„)
- ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
- ì—…ë¡œë“œ ì¼ì‹œì •ì§€/ì¬ê°œ ê°€ëŠ¥

### ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ
1. ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì„ íƒ ë˜ëŠ” ë“œë˜ê·¸
2. íŒŒì¼ íƒ€ì…ë³„ë¡œ ìë™ ë¶„ë¥˜
3. ê° íŒŒì¼ì˜ ì—…ë¡œë“œ ì§„í–‰ë¥  ê°œë³„ í‘œì‹œ
4. ëª¨ë“  íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ í›„ Job ì œì¶œ ê°€ëŠ¥

## í…œí”Œë¦¿ ê´€ë¦¬

### í…œí”Œë¦¿ ìƒì„±
1. **"Templates"** ë©”ë‰´ í´ë¦­
2. **"Create Template"** ë²„íŠ¼ í´ë¦­
3. í…œí”Œë¦¿ ì •ë³´ ì…ë ¥:
   - Name
   - Category
   - Description
   - Required files
   - Slurm settings
4. **"Save"** í´ë¦­

### í…œí”Œë¦¿ ê³µìœ 
- **Private**: ë³¸ì¸ë§Œ ì‚¬ìš© ê°€ëŠ¥
- **Public**: ëª¨ë“  ì‚¬ìš©ìê°€ ì‚¬ìš© ê°€ëŠ¥

### YAML í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
1. ì™¸ë¶€ì—ì„œ ì‘ì„±í•œ YAML íŒŒì¼ ì¤€ë¹„
2. **"Import Template"** í´ë¦­
3. YAML íŒŒì¼ ì„ íƒ
4. ìë™ìœ¼ë¡œ DBì— ì €ì¥ ë° Hot Reload

## API ì‚¬ìš©ë²•

### API Key ìƒì„±
1. í”„ë¡œí•„ ì„¤ì • â†’ **"API Keys"** í´ë¦­
2. **"Create New Key"** í´ë¦­
3. Key ì´ë¦„ ë° ê¶Œí•œ ì„¤ì •
4. ìƒì„±ëœ Key ë³µì‚¬ (âš ï¸ í•œ ë²ˆë§Œ í‘œì‹œë¨)

### API í˜¸ì¶œ ì˜ˆì œ

**íŒŒì¼ ì—…ë¡œë“œ (cURL):**
```bash
# 1. ì—…ë¡œë“œ ì„¸ì…˜ ì´ˆê¸°í™”
curl -X POST https://api.example.com/api/v2/files/upload/init \
  -H "X-API-Key: sk_live_YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "filename": "dataset.tar.gz",
    "file_size": 10485760,
    "user_id": "your_user_id"
  }'

# 2. íŒŒì¼ ì—…ë¡œë“œ (ë‹¨ì¼ ì²­í¬)
curl -X POST https://api.example.com/api/v2/files/upload/chunk \
  -H "X-API-Key: sk_live_YOUR_API_KEY" \
  -F "upload_id=YOUR_UPLOAD_ID" \
  -F "chunk_index=0" \
  -F "chunk=@file.dat"

# 3. ì—…ë¡œë“œ ì™„ë£Œ
curl -X POST https://api.example.com/api/v2/files/upload/complete \
  -H "X-API-Key: sk_live_YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "upload_id": "YOUR_UPLOAD_ID"
  }'
```

**Python ì˜ˆì œ:**
```python
import requests

API_KEY = "sk_live_YOUR_API_KEY"
BASE_URL = "https://api.example.com"

headers = {
    "X-API-Key": API_KEY,
    "Content-Type": "application/json"
}

# ì—…ë¡œë“œ ì´ˆê¸°í™”
response = requests.post(
    f"{BASE_URL}/api/v2/files/upload/init",
    headers=headers,
    json={
        "filename": "dataset.tar.gz",
        "file_size": 1024000,
        "user_id": "user_123"
    }
)

upload_id = response.json()["upload_id"]
print(f"Upload ID: {upload_id}")
```

## ë¬¸ì œ í•´ê²°

### ì—…ë¡œë“œê°€ ëŠë¦° ê²½ìš°
- ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸
- íŒŒì¼ì„ ì••ì¶•í•˜ì—¬ í¬ê¸° ì¤„ì´ê¸°
- ì²­í¬ í¬ê¸° ì¡°ì • (ê¸°ë³¸ 5MB)

### Jobì´ ì œì¶œë˜ì§€ ì•ŠëŠ” ê²½ìš°
- í•„ìˆ˜ íŒŒì¼ì´ ëª¨ë‘ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
- Partition ì„¤ì • í™•ì¸ (compute/viz)
- í• ë‹¹ëŸ‰(Quota) í™•ì¸

### í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
- ì¹´í…Œê³ ë¦¬ í•„í„° í™•ì¸
- Private/Public ì„¤ì • í™•ì¸
- ê²€ìƒ‰ í‚¤ì›Œë“œ ë³€ê²½

## ì§€ì›

ë¬¸ì œê°€ ê³„ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”:
- Email: support@example.com
- Slack: #cluster-support
```

### 6.6 ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Backend ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± (ì»¤ë²„ë¦¬ì§€ 80%+)
- [ ] Frontend ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] E2E í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„±
- [ ] Playwright E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] OpenAPI ìŠ¤í™ ì‘ì„±
- [ ] Swagger UI ë°°í¬
- [ ] User Guide ì‘ì„±
- [ ] API Documentation ì‘ì„±
- [ ] Deployment Guide ì‘ì„±
- [ ] Troubleshooting Guide ì‘ì„±
- [ ] ëª¨ë“  Phase í†µí•© í…ŒìŠ¤íŠ¸
- [ ] Production ë°°í¬ ì¤€ë¹„

---

## ğŸ“… ì „ì²´ ì¼ì • ìš”ì•½

| Phase | ëª©í‘œ | ì˜ˆìƒ ê¸°ê°„ | ìš°ì„ ìˆœìœ„ | ìƒíƒœ |
|-------|------|-----------|----------|------|
| Phase 1 | Apptainer Discovery | 2ì£¼ | âœ… ì™„ë£Œ | 2025-11-05 |
| Phase 2 | Template Management | 2ì£¼ | âœ… ì™„ë£Œ | 2025-11-05 |
| Phase 3 | File Upload (Backend) | 1.5ì£¼ | âœ… ì™„ë£Œ | 2025-11-05 |
| **Phase 3+** | **File Upload (Frontend)** | **3-4ì¼** | **ğŸ”¥ ìµœìš°ì„ ** | âŒ ëŒ€ê¸° |
| Phase 4 | Security & Infrastructure | 2ì£¼ | 2 | âŒ ëŒ€ê¸° |
| Phase 5 | Performance Optimization | 1.5ì£¼ | 3 | âŒ ëŒ€ê¸° |
| Phase 6 | Testing & Documentation | 1ì£¼ | ìµœì¢… | âŒ ëŒ€ê¸° |

**ì´ ì˜ˆìƒ ê¸°ê°„ (Phase 3+~6):** ì•½ 4-5ì£¼

---

## ğŸ¯ ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ ì‘ì—…

### 1ìˆœìœ„: Phase 3 Frontend (3-4ì¼)
Backend APIê°€ ì™„ì„±ë˜ì–´ ë°”ë¡œ Frontend ê°œë°œ ì‹œì‘ ê°€ëŠ¥
- UnifiedUploader ì»´í¬ë„ŒíŠ¸
- ì²­í¬ ì—…ë¡œë“œ UI
- WebSocket ì§„í–‰ë¥  í‘œì‹œ

### 2ìˆœìœ„: Phase 4 Security (2ì£¼)
ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥, ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë³‘ë ¬ ì‘ì—… ê°€ëŠ¥
- JWT Refresh Token
- Redis ì—°ë™
- API Key ì‹œìŠ¤í…œ

### 3ìˆœìœ„: Phase 5 Performance (1.5ì£¼)
ìµœì í™”ëŠ” ê¸°ëŠ¥ ì™„ì„± í›„ ì§„í–‰ ê¶Œì¥
- Backend ë¹„ë™ê¸° ì²˜ë¦¬
- DB ì¿¼ë¦¬ ìµœì í™”
- Frontend Code Splitting

---

## ğŸ“ ê°œë°œ ì‹œì‘ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í™˜ê²½ ì¤€ë¹„
- [ ] Git ìµœì‹  ë²„ì „ìœ¼ë¡œ ì²´í¬ì•„ì›ƒ
- [ ] í˜„ì¬ ì‹œìŠ¤í…œ ë°±ì—… (`git commit`)
- [ ] ê°œë°œ ë¸Œëœì¹˜ ìƒì„± (`git checkout -b phase-X`)
- [ ] Node.js ë° Python ì˜ì¡´ì„± í™•ì¸

### Phase 3 Frontend ì‹œì‘ ì „
- [ ] Backend API í…ŒìŠ¤íŠ¸ ì™„ë£Œ í™•ì¸
- [ ] WebSocket ì„œë²„ ì •ìƒ ë™ì‘ í™•ì¸
- [ ] `/shared/uploads/` ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
- [ ] Frontend ê°œë°œ í™˜ê²½ ì„¤ì •

### Phase 4 Security ì‹œì‘ ì „
- [ ] Redis ì„¤ì¹˜ ê³„íš ìˆ˜ë¦½
- [ ] JWT Secret Key ìƒì„± ë°©ë²• ê²°ì •
- [ ] ê¸°ì¡´ ì¸ì¦ ì‹œìŠ¤í…œ ë¶„ì„
- [ ] API Key ì €ì¥ ë°©ì‹ ì„¤ê³„

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **Job Submit ê¸°ëŠ¥ ê²€ì¦**: ê° Phase ì™„ë£Œ í›„ ë°˜ë“œì‹œ Job Submitì´ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
2. **ë¡¤ë°± ê³„íš**: ê° Phaseë§ˆë‹¤ Git ì»¤ë°‹ìœ¼ë¡œ ë¡¤ë°± í¬ì¸íŠ¸ ìƒì„±
3. **ì ì§„ì  ë°°í¬**: í•œ ë²ˆì— ì—¬ëŸ¬ Phaseë¥¼ ë°°í¬í•˜ì§€ ë§ ê²ƒ
4. **ì‚¬ìš©ì í”¼ë“œë°±**: Phase 3 Frontend ì™„ë£Œ í›„ ì‹¤ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
5. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: Phase 5 ì´ì „ì— í˜„ì¬ ì„±ëŠ¥ ê¸°ì¤€ì¹˜(Baseline) ì¸¡ì •

---

**ë‹¤ìŒ ë‹¨ê³„:** Phase 3 Frontend êµ¬í˜„ ì‹œì‘

```bash
# Phase 3 Frontend ê°œë°œ ì‹œì‘
cd /home/koopark/claude/KooSlurmInstallAutomationRefactory
git checkout -b phase-3-frontend
cd dashboard/frontend_3010
npm install
npm run dev
```
