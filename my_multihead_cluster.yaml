# Multi-Head HPC Cluster Configuration
# Purpose: Full-Copy Quad-Redundancy with Dynamic Clustering
# Version: 2.1
# Generated by: generate_cluster_yaml.sh
#
# 사용 방법:
#   1. 이 파일을 편집하여 모든 설정을 구성합니다
#   2. environment 섹션에서 비밀번호 설정
#   3. 환경변수 설정 불필요! YAML만으로 완전 자동화
#   4. sudo ./setup_cluster_full_multihead.sh 실행
#
# 보안 주의사항:
#   - 민감 정보가 포함되므로 파일 권한 설정: chmod 600 my_multihead_cluster.yaml
#   - Git 저장소에 커밋 시 주의 (.gitignore 권장)

config_version: '2.1'
stage: 0  # Phase 0부터 시작

cluster_info:
  cluster_name: smart-twin-cluster
  domain: hpc.local
  admin_email: admin@example.com
  timezone: Asia/Seoul
  ssh_password: "Soseks314!"  # 모든 노드 공통 SSH 비밀번호

# ============================================================================
# Multi-Head Controllers (N중화 구성)
# ============================================================================
nodes:
  controllers:
    - hostname: icn102-0407-h19
      ip_address: 10.179.100.25
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: 100
      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true
      vip_owner: true
      hardware:
        cpus: 128
        memory_mb: 763152
        disk_gb: 879

    - hostname: icn102-0407-h18
      ip_address: 10.179.100.24
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: 99
      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true
      vip_owner: true
      hardware:
        cpus: 128
        memory_mb: 763152
        disk_gb: 879

    - hostname: icn102-0410-h13
      ip_address: 10.179.100.50
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: 98
      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true
      vip_owner: false
      hardware:
        cpus: 128
        memory_mb: 763152
        disk_gb: 879

  # Compute Nodes
  compute_nodes:
    - hostname: icn102-0407-h14
      ip_address: 10.179.100.20
      ssh_user: koopark
      ssh_port: 22
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0407-h15
      ip_address: 10.179.100.21
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0407-h16
      ip_address: 10.179.100.22
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0407-h17
      ip_address: 10.179.100.23
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h01
      ip_address: 10.179.100.30
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h02
      ip_address: 10.179.100.31
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h03
      ip_address: 10.179.100.32
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763151
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h04
      ip_address: 10.179.100.33
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h05
      ip_address: 10.179.100.34
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h06
      ip_address: 10.179.100.35
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h07
      ip_address: 10.179.100.36
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h08
      ip_address: 10.179.100.37
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h09
      ip_address: 10.179.100.38
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h10
      ip_address: 10.179.100.39
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h01
      ip_address: 10.179.100.40
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h02
      ip_address: 10.179.100.41
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h03
      ip_address: 10.179.100.42
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h04
      ip_address: 10.179.100.43
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h06
      ip_address: 10.179.100.45
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h07
      ip_address: 10.179.100.46
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h08
      ip_address: 10.179.100.47
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h09
      ip_address: 10.179.100.48
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h10
      ip_address: 10.179.100.49
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0410-h13
      ip_address: 10.179.100.50
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: viz
      hardware:
        cpus: 64
        sockets: 2
        cores_per_socket: 32
        threads_per_core: 1
        memory_mb: 505423
        tmp_disk_mb: 416500
        gpus: 0
        gpu_type: nvidia
        gres: gpu:nvidia:0

# ============================================================================
# Virtual IP (VIP) Configuration
# ============================================================================
network:
  management_network: 10.179.100.0/24
  compute_network: 10.179.100.0/24
  vip:
    address: 10.179.100.100
    netmask: 24
    interface: ens18
    vrrp_router_id: 51
    auth_password: hpc_cluster_vip_secret
  firewall:
    enabled: true
    ports:
      slurmd: 6818
      slurmctld: 6817
      slurmdbd: 6819
      auth_backend: 4430
      auth_frontend: 4431
      dashboard_backend: 5010
      websocket: 5011
      cae_backend: 5000
      cae_automation: 5001
      dashboard_frontend: 5173
      app_frontend: 5174
      mariadb: 3306
      redis: 6379
      glusterd: 24007
      glusterd_mgmt: 24008
      gluster_brick_start: 49152
      gluster_brick_end: 49156
      prometheus: 9090
      node_exporter: 9100
      mariadb_exporter: 9104
      redis_exporter: 9121
      ssh: 22
      http: 80
      https: 443
      vrrp: 112

# ============================================================================
# Shared Storage Configuration (GlusterFS)
# ============================================================================
shared_storage:
  type: glusterfs
  glusterfs:
    volume_name: shared_data
    volume_type: replica
    replica_count: auto
    brick_path: /scratch/glusterfs/brick
    mount_point: /mnt/gluster
    options:
      performance.cache-size: 256MB
      network.ping-timeout: 10
      cluster.self-heal-daemon: enable
    directories:
      - frontend_builds
      - slurm/state
      - slurm/logs
      - slurm/spool
      - uploads
      - config
  backup:
    enabled: true
    path: /var/backups/cluster
    schedule:
      full_backup: "0 2 * * *"
      incremental: "0 * * * *"
    retention:
      daily: 7
      weekly: 4
      monthly: 12
    items:
      - configs
      - databases
      - slurm_state
      - glusterfs_meta

# ============================================================================
# Database Configuration (MariaDB Galera Cluster)
# ============================================================================
database:
  enabled: true
  type: mariadb-galera
  mariadb:
    version: "10.6"
    root_password: "Soseks314!"   # MariaDB root 계정
    sst_user: sst_sync                      # SST 동기화 사용자
    sst_password: "Soseks314!"      # SST 동기화 비밀번호
    galera:
      cluster_name: hpc_portal_cluster
      sst_method: rsync
      cluster_address: auto
      slave_threads: 4
    databases:
      - name: slurm_acct_db
        user: slurm
        password: "${DB_SLURM_PASSWORD}"
      - name: auth_portal
        user: auth_user
        password: "${DB_AUTH_PASSWORD}"
    config:
      max_connections: 500
      innodb_buffer_pool_size: 8G
      innodb_log_file_size: 512M

# ============================================================================
# Redis Cluster Configuration
# ============================================================================
redis:
  enabled: true
  type: cluster
  cluster:
    port: 6379
    password: "${REDIS_PASSWORD}"
    cluster_mode: true
    replicas: 0
    nodes: auto
    cluster_node_timeout: 5000
    appendonly: yes
    appendfsync: everysec
    maxmemory: 4gb
    maxmemory_policy: allkeys-lru

# ============================================================================
# Slurm Configuration
# ============================================================================
slurm_config:
  version: 23.02.7
  install_path: /usr/local/slurm
  config_path: /usr/local/slurm/etc
  log_path: /mnt/gluster/slurm/logs
  spool_path: /var/spool/slurm
  state_save_location: /mnt/gluster/slurm/state
  multi_master:
    enabled: true
    controllers: auto
  accounting:
    storage_type: accounting_storage/slurmdbd
    storage_host: 127.0.0.1
    storage_port: 6819
  scheduler:
    type: sched/backfill
    max_job_count: 10000
    max_array_size: 1000
  partitions:
    - name: normal
      nodes: node[001-001]
      default: true
      max_time: 7-00:00:00
      max_nodes: 1
      state: UP
    - name: viz
      nodes: viz-node[001-001]
      default: false
      max_time: "60-00:00:00"
      max_nodes: 1
      state: UP
      gres: gpu:1
      exclusive: false

# ============================================================================
# Web Services Configuration
# ============================================================================
web_services:
  enabled: true
  shared_config_path: /mnt/gluster/config
  frontend_build_path: /mnt/gluster/frontend_builds
  services:
    - name: auth_portal
      port: 4430
      type: backend
      health_check: /health
    - name: auth_frontend
      port: 4431
      type: frontend
      build_path: auth_frontend_4431
    - name: dashboard_backend
      port: 5010
      type: backend
      health_check: /health
    - name: websocket
      port: 5011
      type: backend
      websocket: true
    - name: cae_backend
      port: 5000
      type: backend
      health_check: /health
    - name: cae_automation
      port: 5001
      type: backend
      health_check: /health
    - name: dashboard_frontend
      port: 5173
      type: frontend
      build_path: dashboard_5173
    - name: app_frontend
      port: 5174
      type: frontend
      build_path: app_5174

# ============================================================================
# High Availability Configuration
# ============================================================================
high_availability:
  keepalived:
    enabled: true
    health_check:
      script: /usr/local/bin/check_all_services.sh
      interval: 2
      weight: -20
      fall: 3
      rise: 2
    notifications:
      master: /usr/local/bin/notify_master.sh
      backup: /usr/local/bin/notify_backup.sh
      fault: /usr/local/bin/notify_fault.sh
  auto_recovery:
    enabled: true
    galera:
      enabled: true
      script: /opt/hpc-dashboard/cluster/utils/galera_auto_recover.sh
    redis:
      enabled: true
      rebalance_on_join: true

# ============================================================================
# Monitoring & Alerting
# ============================================================================
monitoring:
  prometheus:
    enabled: true
    port: 9090
    retention_days: 30
    scrape_interval: 15s
  node_exporter:
    enabled: true
    port: 9100
  mariadb_exporter:
    enabled: true
    port: 9104
  redis_exporter:
    enabled: true
    port: 9121
  grafana:
    enabled: true
    port: 3000
    admin_password: "${GRAFANA_PASSWORD}"
  alerting:
    enabled: true
    channels:
      - type: slack
        webhook_url: "${SLACK_WEBHOOK_URL}"
        channel: "#hpc-alerts"
      - type: email
        smtp_host: smtp.gmail.com
        smtp_port: 587
        from: alerts@hpc.local
        to: admin@hpc.local
    rules:
      - alert: NodeDown
        duration: 30s
        severity: critical
      - alert: HighCPU
        threshold: 90
        duration: 5m
        severity: warning
      - alert: HighDisk
        threshold: 80
        duration: 1m
        severity: warning
      - alert: GaleraNodeDown
        duration: 10s
        severity: critical
      - alert: VIPFailover
        duration: 0s
        severity: info

# ============================================================================
# User & Security Configuration
# ============================================================================
users:
  admin_user: admin
  slurm_user: slurm
  slurm_uid: 1001
  slurm_gid: 1001
  munge_user: munge
  munge_uid: 1002
  munge_gid: 1002
  cluster_users:
    - username: user01
      uid: 2001
      gid: 2001
      groups: [users, hpc]

security:
  munge:
    key_rotation_days: 30
  selinux:
    enabled: true
    mode: permissive
  jwt:
    secret_key: "${JWT_SECRET_KEY}"
    algorithm: HS256
    expiration_hours: 8

# ============================================================================
# Container & GPU Support
# ============================================================================
container_support:
  apptainer:
    enabled: true
    version: 1.2.5
    install_path: /usr/local
    image_path: /share/apptainer/images
    cache_path: /tmp/apptainer
    bind_paths: [/home, /share, /scratch, /tmp]
    deployment_strategy: local_copy
    master_images_path: /scratch/apptainers
    node_local_path: /scratch/apptainers

gpu_computing:
  nvidia:
    enabled: true
    nodes: []
  amd:
    enabled: true
    nodes: []

# ============================================================================
# Dynamic Clustering Options
# ============================================================================
clustering:
  auto_discovery:
    enabled: true
    timeout: 5
    health_check_interval: 30
  auto_join:
    enabled: true
  node_management:
    allow_dynamic_add: true
    allow_dynamic_remove: true
    min_controllers: 1
    max_controllers: 10
  status_format: table

# ============================================================================
# Environment Variables (Template)
# ============================================================================
environment:
  DB_ROOT_PASSWORD: "Soseks314!"
  DB_SST_PASSWORD: "Soseks314!"
  DB_SLURM_PASSWORD: "Soseks314!"
  DB_AUTH_PASSWORD: "Soseks314!"
  REDIS_PASSWORD: "Soseks314!"
  JWT_SECRET_KEY: "Soseks314!"
  GRAFANA_PASSWORD: "Soseks314!"
  SLACK_WEBHOOK_URL: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# ============================================================================
# Installation & Deployment
# ============================================================================
installation:
  install_method: package
  offline_mode: true
  offline_package_dir: "./offline_packages"
  package_cache_path: /var/cache/cluster_packages
  auto_confirm: false
  skip_munge: false
  skip_slurm_base: false
  install_mpi: false
  install_apptainer: false
  timeouts:
    cluster_join_wait: 300
    service_start_wait: 30
    ssh_timeout: 10
    health_check_interval: 10
  phases:
    phase0_storage: true
    phase1_database: true
    phase2_redis: true
    phase3_slurm: true
    phase4_keepalived: true
    phase5_web: true
    phase6_monitoring: true

# ============================================================================
# Time Synchronization
# ============================================================================
time_synchronization:
  enabled: true
  ntp_servers:
    - 10.198.112.202
    - time.google.com
    - time.cloudflare.com
    - pool.ntp.org
  timezone: Asia/Seoul
