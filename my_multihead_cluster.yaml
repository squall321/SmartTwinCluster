# Multi-Head HPC Cluster Configuration
# Purpose: Full-Copy Quad-Redundancy with Dynamic Clustering
# Version: 2.1
# Generated by: generate_cluster_yaml.sh
#
# 사용 방법:
#   1. 이 파일을 편집하여 모든 설정을 구성합니다
#   2. environment 섹션에서 비밀번호 설정
#   3. 환경변수 설정 불필요! YAML만으로 완전 자동화
#   4. sudo ./setup_cluster_full_multihead.sh 실행
#
# 보안 주의사항:
#   - 민감 정보가 포함되므로 파일 권한 설정: chmod 600 my_multihead_cluster.yaml
#   - Git 저장소에 커밋 시 주의 (.gitignore 권장)

config_version: '2.1'
stage: 0  # Phase 0부터 시작

cluster_info:
  cluster_name: smart-twin-cluster
  domain: hpc.local
  admin_email: admin@example.com
  timezone: Asia/Seoul
  ssh_password: "Soseks314!"  # 모든 노드 공통 SSH 비밀번호

# ============================================================================
# Multi-Head Controllers (N중화 구성)
# ============================================================================
nodes:
  controllers:
    - hostname: icn102-0407-h19
      ip_address: 10.179.100.25
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: 100
      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true
      vip_owner: true
      hardware:
        cpus: 128
        memory_mb: 763152
        disk_gb: 879

    - hostname: icn102-0407-h18
      ip_address: 10.179.100.24
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: 99
      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true
      vip_owner: true
      hardware:
        cpus: 128
        memory_mb: 763152
        disk_gb: 879

    - hostname: icn102-0410-h13
      ip_address: 10.179.100.50
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: 98
      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true
      vip_owner: false
      hardware:
        cpus: 128
        memory_mb: 763152
        disk_gb: 879

  # Compute Nodes
  compute_nodes:
    - hostname: icn102-0407-h14
      ip_address: 10.179.100.20
      ssh_user: koopark
      ssh_port: 22
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0407-h15
      ip_address: 10.179.100.21
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0407-h16
      ip_address: 10.179.100.22
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0407-h17
      ip_address: 10.179.100.23
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h01
      ip_address: 10.179.100.30
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h02
      ip_address: 10.179.100.31
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h03
      ip_address: 10.179.100.32
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763151
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h04
      ip_address: 10.179.100.33
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h05
      ip_address: 10.179.100.34
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h06
      ip_address: 10.179.100.35
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h07
      ip_address: 10.179.100.36
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h08
      ip_address: 10.179.100.37
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h09
      ip_address: 10.179.100.38
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0408-h10
      ip_address: 10.179.100.39
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h01
      ip_address: 10.179.100.40
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h02
      ip_address: 10.179.100.41
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h03
      ip_address: 10.179.100.42
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h04
      ip_address: 10.179.100.43
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h06
      ip_address: 10.179.100.45
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h07
      ip_address: 10.179.100.46
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h08
      ip_address: 10.179.100.47
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h09
      ip_address: 10.179.100.48
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0409-h10
      ip_address: 10.179.100.49
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: 128
        sockets: 2
        cores_per_socket: 64
        threads_per_core: 1
        memory_mb: 763152
        tmp_disk_mb: 439500

    - hostname: icn102-0410-h13
      ip_address: 10.179.100.50
      ssh_user: koopark
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: viz
      hardware:
        cpus: 64
        sockets: 2
        cores_per_socket: 32
        threads_per_core: 1
        memory_mb: 505423
        tmp_disk_mb: 416500
        gpus: 0
        gpu_type: nvidia
        gres: gpu:nvidia:0

# ============================================================================
# Virtual IP (VIP) Configuration
# ============================================================================
network:
  management_network: 10.179.100.0/24
  compute_network: 10.179.100.0/24
  vip:
    address: 10.179.100.100
    netmask: 24
    interface: ens18
    vrrp_router_id: 51
    auth_password: hpc_cluster_vip_secret
  firewall:
    enabled: true
    ports:
      slurmd: 6818
      slurmctld: 6817
      slurmdbd: 6819
      auth_backend: 4430
      auth_frontend: 4431
      dashboard_backend: 5010
      websocket: 5011
      cae_backend: 5000
      cae_automation: 5001
      dashboard_frontend: 5173
      app_frontend: 5174
      mariadb: 3306
      redis: 6379
      glusterd: 24007
      glusterd_mgmt: 24008
      gluster_brick_start: 49152
      gluster_brick_end: 49156
      prometheus: 9090
      node_exporter: 9100
      mariadb_exporter: 9104
      redis_exporter: 9121
      ssh: 22
      http: 80
      https: 443
      vrrp: 112

# ============================================================================
# Shared Storage Configuration (GlusterFS)
# ============================================================================
shared_storage:
  type: glusterfs
  glusterfs:
    volume_name: shared_data
    volume_type: replica
    replica_count: auto
    brick_path: /scratch/glusterfs/brick
    mount_point: /mnt/gluster
    options:
      performance.cache-size: 256MB
      network.ping-timeout: 10
      cluster.self-heal-daemon: enable
    directories:
      - frontend_builds
      - slurm/state
      - slurm/logs
      - slurm/spool
      - uploads
      - config
  backup:
    enabled: true
    path: /var/backups/cluster
    schedule:
      full_backup: "0 2 * * *"
      incremental: "0 * * * *"
    retention:
      daily: 7
      weekly: 4
      monthly: 12
    items:
      - configs
      - databases
      - slurm_state
      - glusterfs_meta

# ============================================================================
# Database Configuration (MariaDB Galera Cluster)
# ============================================================================
database:
  enabled: true
  type: mariadb-galera
  mariadb:
    version: "10.6"
    root_password: "Soseks314!"   # MariaDB root 계정
    sst_user: sst_sync                      # SST 동기화 사용자
    sst_password: "Soseks314!"      # SST 동기화 비밀번호
    galera:
      cluster_name: hpc_portal_cluster
      sst_method: rsync
      cluster_address: auto
      slave_threads: 4
    databases:
      - name: slurm_acct_db
        user: slurm
        password: "${DB_SLURM_PASSWORD}"
      - name: auth_portal
        user: auth_user
        password: "${DB_AUTH_PASSWORD}"
    config:
      max_connections: 500
      innodb_buffer_pool_size: 8G
      innodb_log_file_size: 512M

# ============================================================================
# Redis Cluster Configuration
# ============================================================================
redis:
  enabled: true
  type: cluster
  cluster:
    port: 6379
    password: "${REDIS_PASSWORD}"
    cluster_mode: true
    replicas: 0
    nodes: auto
    cluster_node_timeout: 5000
    appendonly: yes
    appendfsync: everysec
    maxmemory: 4gb
    maxmemory_policy: allkeys-lru

# ============================================================================
# Slurm Configuration
# ============================================================================
slurm_config:
  version: 23.02.7
  install_path: /usr/local/slurm
  config_path: /usr/local/slurm/etc
  log_path: /mnt/gluster/slurm/logs
  spool_path: /var/spool/slurm
  state_save_location: /mnt/gluster/slurm/state
  multi_master:
    enabled: true
    controllers: auto
  accounting:
    storage_type: accounting_storage/slurmdbd
    storage_host: 127.0.0.1
    storage_port: 6819
  scheduler:
    type: sched/backfill
    max_job_count: 10000
    max_array_size: 1000
  partitions:
    - name: normal
      nodes: node[001-001]
      default: true
      max_time: 7-00:00:00
      max_nodes: 1
      state: UP
    - name: viz
      nodes: viz-node[001-001]
      default: false
      max_time: "60-00:00:00"
      max_nodes: 1
      state: UP
      gres: gpu:1
      exclusive: false

# ============================================================================
# Web Services Configuration
# ============================================================================
web_services:
  enabled: true
  shared_config_path: /mnt/gluster/config
  frontend_build_path: /mnt/gluster/frontend_builds
  services:
    - name: auth_portal
      port: 4430
      type: backend
      health_check: /health
    - name: auth_frontend
      port: 4431
      type: frontend
      build_path: auth_frontend_4431
    - name: dashboard_backend
      port: 5010
      type: backend
      health_check: /health
    - name: websocket
      port: 5011
      type: backend
      websocket: true
    - name: cae_backend
      port: 5000
      type: backend
      health_check: /health
    - name: cae_automation
      port: 5001
      type: backend
      health_check: /health
    - name: dashboard_frontend
      port: 5173
      type: frontend
      build_path: dashboard_5173
    - name: app_frontend
      port: 5174
      type: frontend
      build_path: app_5174

# ============================================================================
# High Availability Configuration
# ============================================================================
high_availability:
  keepalived:
    enabled: true
    health_check:
      script: /usr/local/bin/check_all_services.sh
      interval: 2
      weight: -20
      fall: 3
      rise: 2
    notifications:
      master: /usr/local/bin/notify_master.sh
      backup: /usr/local/bin/notify_backup.sh
      fault: /usr/local/bin/notify_fault.sh
  auto_recovery:
    enabled: true
    galera:
      enabled: true
      script: /opt/hpc-dashboard/cluster/utils/galera_auto_recover.sh
    redis:
      enabled: true
      rebalance_on_join: true

# ============================================================================
# Monitoring & Alerting
# ============================================================================
monitoring:
  prometheus:
    enabled: true
    port: 9090
    retention_days: 30
    scrape_interval: 15s
  node_exporter:
    enabled: true
    port: 9100
  mariadb_exporter:
    enabled: true
    port: 9104
  redis_exporter:
    enabled: true
    port: 9121
  grafana:
    enabled: true
    port: 3000
    admin_password: "${GRAFANA_PASSWORD}"
  alerting:
    enabled: true
    channels:
      - type: slack
        webhook_url: "${SLACK_WEBHOOK_URL}"
        channel: "#hpc-alerts"
      - type: email
        smtp_host: smtp.gmail.com
        smtp_port: 587
        from: alerts@hpc.local
        to: admin@hpc.local
    rules:
      - alert: NodeDown
        duration: 30s
        severity: critical
      - alert: HighCPU
        threshold: 90
        duration: 5m
        severity: warning
      - alert: HighDisk
        threshold: 80
        duration: 1m
        severity: warning
      - alert: GaleraNodeDown
        duration: 10s
        severity: critical
      - alert: VIPFailover
        duration: 0s
        severity: info

# ============================================================================
# User & Security Configuration
# ============================================================================
users:
  admin_user: admin
  slurm_user: slurm
  slurm_uid: 64001
  slurm_gid: 64001
  munge_user: munge
  munge_uid: 64002
  munge_gid: 64002
  cluster_users:
    - username: user01
      uid: 65001
      gid: 65001
      groups: [users, hpc]

security:
  munge:
    key_rotation_days: 30
  selinux:
    enabled: true
    mode: permissive
  jwt:
    secret_key: "${JWT_SECRET_KEY}"
    algorithm: HS256
    expiration_hours: 8

# ============================================================================
# Cgroup Resource Control (Ubuntu 22.04 uses cgroup v2 by default)
# ============================================================================
cgroup:
  enabled: true                # cgroup 활성화 여부
  constrain_ram: true          # 메모리 제한 (ConstrainRAMSpace)
  constrain_swap: true         # 스왑 제한 (ConstrainSwapSpace)
  constrain_devices: true      # 디바이스 제한 - GPU 접근 제어 (ConstrainDevices)
  constrain_cores: true        # CPU 친화성 (ConstrainCores)
  allowed_ram_percent: 100     # 허용 RAM 비율 (100 = 엄격 제한)
  allowed_swap_percent: 0      # 허용 스왑 비율 (0 = 스왑 사용 안함)

# ============================================================================
# Container & GPU Support
# ============================================================================
container_support:
  apptainer:
    enabled: true
    version: 1.2.5
    install_path: /usr/local
    image_path: /share/apptainer/images
    cache_path: /tmp/apptainer
    bind_paths: [/home, /share, /scratch, /tmp]

    # SIF 이미지 배포 전략
    # - skip: 이미 존재하면 건너뜀 (기본값, 빠름)
    # - overwrite: 무조건 덮어쓰기 (느림)
    # - update: 크기/날짜 비교 후 변경된 것만 업데이트 (중간)
    sif_deployment_mode: skip  # skip | overwrite | update

    deployment_strategy: local_copy
    master_images_path: /scratch/apptainers
    node_local_path: /scratch/apptainers

gpu_computing:
  nvidia:
    enabled: true
    nodes: []
  amd:
    enabled: true
    nodes: []

# ============================================================================
# Dynamic Clustering Options
# ============================================================================
clustering:
  auto_discovery:
    enabled: true
    timeout: 5
    health_check_interval: 30
  auto_join:
    enabled: true
  node_management:
    allow_dynamic_add: true
    allow_dynamic_remove: true
    min_controllers: 1
    max_controllers: 10
  status_format: table

# ============================================================================
# Environment Variables (Template)
# ============================================================================
environment:
  DB_ROOT_PASSWORD: "Soseks314!"
  DB_SST_PASSWORD: "Soseks314!"
  DB_SLURM_PASSWORD: "Soseks314!"
  DB_AUTH_PASSWORD: "Soseks314!"
  REDIS_PASSWORD: "Soseks314!"
  JWT_SECRET_KEY: "Soseks314!"
  GRAFANA_PASSWORD: "Soseks314!"
  SLACK_WEBHOOK_URL: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# ============================================================================
# Installation & Deployment
# ============================================================================
installation:
  install_method: package
  offline_mode: true
  offline_package_dir: "./offline_packages"
  package_cache_path: /var/cache/cluster_packages
  auto_confirm: false
  skip_munge: false
  skip_slurm_base: false
  install_mpi: false
  install_apptainer: false
  timeouts:
    cluster_join_wait: 300
    service_start_wait: 30
    ssh_timeout: 10
    health_check_interval: 10
  phases:
    phase0_storage: true
    phase1_database: true
    phase2_redis: true
    phase3_slurm: true
    phase4_keepalived: true
    phase5_web: true
    phase6_monitoring: true

# ============================================================================
# Time Synchronization
# ============================================================================
time_synchronization:
  enabled: true
  ntp_servers:
    - 10.198.112.202
    - time.google.com
    - time.cloudflare.com
    - pool.ntp.org
  timezone: Asia/Seoul

# ============================================================================
# Web/SSL Settings
# ============================================================================
web:
  # 외부 접속 주소 (브라우저에서 접속할 주소)
  # - 공인 IP 또는 도메인
  # - 내부 접속만 하려면 VIP 주소 사용: 10.179.100.100
  # - 외부 접속이 필요하면 공인 IP 또는 도메인 입력: 110.15.177.120 또는 cluster.example.com
  public_url: "110.15.177.120"  # 외부 접속 주소 (공인 IP 또는 도메인)

  ssl:
    # SSL 인증서 모드: letsencrypt | self_signed | none
    # - letsencrypt: Let's Encrypt 인증서 (인터넷 연결 필요)
    # - self_signed: 자체 서명 인증서 (오프라인 환경용, 기본값)
    # - none: SSL 비활성화 (HTTP만 사용)
    mode: self_signed
    # Let's Encrypt 사용 시 도메인 (letsencrypt 모드 필수)
    domain: ""
    # Let's Encrypt 사용 시 이메일 (인증서 만료 알림용)
    email: ""

# ============================================================================
# SSO Configuration (SAML / OIDC)
# ============================================================================
sso:
  # SSO 활성화 여부 (false면 Mock IdP 사용 - 개발/테스트용)
  enabled: false

  # SSO 타입: saml | oidc
  type: saml

  # ==========================================================================
  # SAML 설정 (type: saml 일 때 사용)
  # ==========================================================================
  saml:
    # IdP 메타데이터 URL (보통 이걸로 자동 설정 가능)
    idp_metadata_url: ""

    # IdP 개별 설정 (메타데이터 URL이 없는 경우)
    idp:
      entity_id: ""
      sso_url: ""
      slo_url: ""
      # IdP 인증서 (PEM 형식, 또는 파일 경로)
      certificate: ""
      # certificate_file: "/path/to/idp-cert.pem"

    # SP(Service Provider) 설정 - 우리 시스템 정보
    sp:
      entity_id: "hpc-dashboard"  # SSO 담당자에게 등록할 SP ID

  # ==========================================================================
  # OIDC 설정 (type: oidc 일 때 사용)
  # ==========================================================================
  oidc:
    issuer: ""                    # https://sso.company.com
    client_id: ""                 # SSO 담당자에게 받은 Client ID
    client_secret: ""             # SSO 담당자에게 받은 Client Secret
    # 요청할 scope (기본값: openid,profile,email,groups)
    scopes:
      - openid
      - profile
      - email
      - groups

    # 명시적 클레임 요청 (SAML의 AttributeConsumingService와 유사)
    # OIDC는 기본적으로 scope로 클레임을 요청하지만,
    # claims 파라미터로 개별 클레임을 명시적으로 요청할 수 있습니다.
    # IdP가 claims 파라미터를 지원해야 합니다.
    # 비워두면 attribute_mapping의 값들로 자동 생성됩니다.
    requested_claims: []
    # requested_claims:
    #   - preferred_username
    #   - email
    #   - groups
    #   - department

  # ==========================================================================
  # SSO 속성 매핑 (SAML/OIDC 공통)
  # 사내 SSO가 보내는 속성명 → 내부 속성명
  # ==========================================================================
  attribute_mapping:
    # 사용자 ID (필수) - SAML: uid, OIDC: preferred_username
    username: "uid"
    # 이메일 (필수)
    email: "email"
    # 그룹/부서 (권한 매핑용, 필수) - SAML: memberOf, OIDC: groups
    groups: "groups"
    # 선택적 속성
    display_name: "displayName"
    department: "department"

  # ==========================================================================
  # 그룹별 권한 매핑
  # 사내 그룹명 → 서비스 권한 (dashboard, cae, vnc, app, admin)
  # ==========================================================================
  group_permissions:
    # 관리자 그룹 (모든 권한)
    "HPC-Admins":
      - dashboard
      - cae
      - vnc
      - app
      - admin

    # DX팀 (대시보드 + VNC + 앱)
    "DX-Users":
      - dashboard
      - vnc
      - app

    # CAE팀 (대시보드 + CAE + VNC + 앱)
    "CAEG-Users":
      - dashboard
      - cae
      - vnc
      - app

    # AD 그룹 DN 형식 예시 (사내 SSO에서 전체 DN으로 보내는 경우)
    # "CN=HPC-Admins,OU=Groups,DC=company,DC=com":
    #   - dashboard
    #   - cae
    #   - vnc
    #   - app
    #   - admin

# ============================================================================
# SAML IdP Test Users (개발/테스트용 - sso.enabled: false 일 때만 사용)
# ============================================================================
saml:
  test_users:
    - username: koopark
      password: "Soseks314!"
      email: koopark@hpc.local
      roles:
        - admin
        - user
