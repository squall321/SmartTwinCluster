# YAML ê¸°ë°˜ Slurm ì„¤ì • ìë™í™”

## ğŸ“‹ ê°œìš”

ëª¨ë“  Slurm ì„¤ì •ì„ `my_cluster.yaml`ì—ì„œ ì½ì–´ì™€ì„œ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
ë” ì´ìƒ í•˜ë“œì½”ë”©ëœ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤! âœ¨

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### âœ… YAMLì—ì„œ ìë™ìœ¼ë¡œ ì½ì–´ì˜¤ëŠ” í•­ëª©ë“¤

1. **í´ëŸ¬ìŠ¤í„° ì •ë³´**
   - ClusterName
   - SlurmctldHost (Controller í˜¸ìŠ¤íŠ¸ëª… ë° IP)

2. **ë…¸ë“œ ì¬ë¶€íŒ…**
   - âœ¨ **RebootProgram** (YAMLì˜ `slurm_config.reboot_program`)

3. **ê³„ì‚° ë…¸ë“œ**
   - NodeName, NodeAddr (ë™ì  ìƒì„±)
   - CPUs, Sockets, CoresPerSocket, ThreadsPerCore
   - RealMemory

4. **íŒŒí‹°ì…˜**
   - PartitionName, Nodes (ë™ì  ìƒì„±)
   - Default, MaxTime, MaxNodes, State

5. **ê²½ë¡œ ì„¤ì •**
   - install_path
   - config_path
   - log_path
   - spool_path

6. **ìŠ¤ì¼€ì¤„ëŸ¬**
   - SchedulerType
   - Accounting ì„¤ì •

## ğŸ“‚ ìƒì„±ë˜ëŠ” íŒŒì¼

```
/usr/local/slurm/etc/
â”œâ”€â”€ slurm.conf      â† YAML ê¸°ë°˜ ë™ì  ìƒì„±
â””â”€â”€ cgroup.conf     â† YAML ê¸°ë°˜ ë™ì  ìƒì„±

/etc/systemd/system/
â”œâ”€â”€ slurmctld.service  â† YAML ê²½ë¡œ ì‚¬ìš©
â””â”€â”€ slurmd.service     â† YAML ê²½ë¡œ ì‚¬ìš©

/etc/tmpfiles.d/
â””â”€â”€ slurm.conf
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ë°©ë²• 1: Python ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ (ê¶Œì¥)

```bash
cd /home/koopark/claude/KooSlurmInstallAutomationRefactory

# 1. ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x configure_slurm_from_yaml.py

# 2. ë¯¸ë¦¬ë³´ê¸° (ì„¤ì • í™•ì¸ë§Œ)
python3 configure_slurm_from_yaml.py --dry-run

# 3. ì‹¤ì œ ìƒì„±
python3 configure_slurm_from_yaml.py

# 4. ë‹¤ë¥¸ YAML íŒŒì¼ ì‚¬ìš©
python3 configure_slurm_from_yaml.py -c custom_cluster.yaml
```

### ë°©ë²• 2: Bash ë˜í¼ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

```bash
chmod +x configure_slurm_cgroup_v2_YAML.sh
./configure_slurm_cgroup_v2_YAML.sh
```

### ë°©ë²• 3: setup_cluster_full.shì— í†µí•©

```bash
# setup_cluster_full.sh íŒ¨ì¹˜ (Step 8ì„ YAML ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½)
chmod +x patch_setup_cluster_full.sh
./patch_setup_cluster_full.sh

# ì „ì²´ ì„¤ì¹˜ ì‹¤í–‰
./setup_cluster_full.sh
```

## ğŸ“ ì„¤ì • ì˜ˆì‹œ (my_cluster.yaml)

```yaml
cluster_info:
  cluster_name: mini-cluster

nodes:
  controller:
    hostname: smarttwincluster
    ip_address: 192.168.122.1
  
  compute_nodes:
    - hostname: node001
      ip_address: 192.168.122.90
      hardware:
        cpus: 2
        sockets: 1
        cores_per_socket: 2
        threads_per_core: 1
        memory_mb: 4096

slurm_config:
  version: 22.05.8
  install_path: /usr/local/slurm
  config_path: /usr/local/slurm/etc
  log_path: /var/log/slurm
  
  # âœ¨ ì¤‘ìš”: RebootProgram ì„¤ì •
  reboot_program: /sbin/reboot
  
  scheduler:
    type: sched/backfill
  
  accounting:
    storage_type: accounting_storage/none
  
  partitions:
    - name: normal
      nodes: node[1-2]
      default: true
      max_time: 7-00:00:00
```

## ğŸ” ìƒì„±ëœ ì„¤ì • í™•ì¸

```bash
# 1. slurm.conf í™•ì¸
cat /usr/local/slurm/etc/slurm.conf

# 2. RebootProgram ì„¤ì • í™•ì¸
grep "^RebootProgram" /usr/local/slurm/etc/slurm.conf

# ì˜ˆìƒ ì¶œë ¥:
# RebootProgram=/sbin/reboot

# 3. ë…¸ë“œ ì„¤ì • í™•ì¸
grep "^NodeName" /usr/local/slurm/etc/slurm.conf

# 4. íŒŒí‹°ì…˜ ì„¤ì • í™•ì¸
grep "^PartitionName" /usr/local/slurm/etc/slurm.conf

# 5. scontrolë¡œ í™•ì¸
scontrol show config | grep RebootProgram
```

## ğŸ“¤ ê³„ì‚° ë…¸ë“œì— ë°°í¬

```bash
# ìë™ìœ¼ë¡œ ìƒì„±ëœ ë…¸ë“œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°í¬

# node001
scp /usr/local/slurm/etc/slurm.conf koopark@192.168.122.90:/tmp/
ssh koopark@192.168.122.90 'sudo mv /tmp/slurm.conf /usr/local/slurm/etc/ && sudo chown slurm:slurm /usr/local/slurm/etc/slurm.conf'

# node002
scp /usr/local/slurm/etc/slurm.conf koopark@192.168.122.103:/tmp/
ssh koopark@192.168.122.103 'sudo mv /tmp/slurm.conf /usr/local/slurm/etc/ && sudo chown slurm:slurm /usr/local/slurm/etc/slurm.conf'
```

ë˜ëŠ” ìë™ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸:

```bash
# sync_config_to_nodes.shë¥¼ ì‚¬ìš© (ì´ë¯¸ ì¡´ì¬)
./sync_config_to_nodes.sh
```

## ğŸ”„ Slurm ì¬ì‹œì‘

```bash
# ì»¨íŠ¸ë¡¤ëŸ¬
sudo systemctl restart slurmctld

# ê³„ì‚° ë…¸ë“œ
ssh koopark@192.168.122.90 'sudo systemctl restart slurmd'
ssh koopark@192.168.122.103 'sudo systemctl restart slurmd'
```

## âœ… ì¥ì 

### âŒ ì´ì „ (configure_slurm_cgroup_v2.sh)
- í•˜ë“œì½”ë”©ëœ ë…¸ë“œ ì •ë³´
- í•˜ë“œì½”ë”©ëœ IP ì£¼ì†Œ
- RebootProgram ì„¤ì • ì—†ìŒ
- YAML ë³€ê²½í•´ë„ ë°˜ì˜ ì•ˆë¨

### âœ… í˜„ì¬ (configure_slurm_from_yaml.py)
- ëª¨ë“  ì„¤ì •ì„ YAMLì—ì„œ ì½ìŒ
- ë…¸ë“œ ì¶”ê°€/ì œê±°ê°€ YAMLë§Œ ìˆ˜ì •í•˜ë©´ ë¨
- RebootProgram ìë™ ë°˜ì˜
- íŒŒí‹°ì…˜ë„ ë™ì  ìƒì„±
- í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# 1. DRY RUNìœ¼ë¡œ ë¯¸ë¦¬ë³´ê¸°
python3 configure_slurm_from_yaml.py --dry-run

# 2. ì‹¤ì œ ìƒì„±
sudo python3 configure_slurm_from_yaml.py

# 3. RebootProgram í™•ì¸
grep RebootProgram /usr/local/slurm/etc/slurm.conf

# 4. ë…¸ë“œ ì¬ë¶€íŒ… í…ŒìŠ¤íŠ¸ (ì£¼ì˜!)
scontrol reboot node001 reason="yaml config test"
```

## ğŸ“‹ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q: YAML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì˜¤ë¥˜
```bash
# A: my_cluster.yaml íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
ls -l my_cluster.yaml

# ì—†ìœ¼ë©´ ìƒì„±
cp examples/2node_example.yaml my_cluster.yaml
vim my_cluster.yaml
```

### Q: Permission denied ì˜¤ë¥˜
```bash
# A: sudoë¡œ ì‹¤í–‰
sudo python3 configure_slurm_from_yaml.py
```

### Q: slurm ì‚¬ìš©ìê°€ ì—†ë‹¤ëŠ” ì˜¤ë¥˜
```bash
# A: slurm ì‚¬ìš©ì ìƒì„±
sudo groupadd -g 1001 slurm
sudo useradd -u 1001 -g 1001 -m -s /bin/bash slurm
```

### Q: RebootProgramì´ ë°˜ì˜ë˜ì§€ ì•ŠìŒ
```bash
# A: YAML íŒŒì¼ í™•ì¸
grep reboot_program my_cluster.yaml

# ì—†ìœ¼ë©´ ì¶”ê°€
# slurm_config:
#   reboot_program: /sbin/reboot

# ë‹¤ì‹œ ìƒì„±
python3 configure_slurm_from_yaml.py
```

## ğŸ¯ í•µì‹¬ ìš”ì•½

1. **ëª¨ë“  ì„¤ì •ì€ my_cluster.yamlì—ì„œ!**
2. **ë” ì´ìƒ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • í•„ìš” ì—†ìŒ**
3. **RebootProgram ìë™ ë°˜ì˜**
4. **ë…¸ë“œ/íŒŒí‹°ì…˜ ë™ì  ìƒì„±**

## ğŸ“š ê´€ë ¨ íŒŒì¼

- `configure_slurm_from_yaml.py` - ë©”ì¸ Python ìŠ¤í¬ë¦½íŠ¸
- `configure_slurm_cgroup_v2_YAML.sh` - Bash ë˜í¼
- `my_cluster.yaml` - ì„¤ì • íŒŒì¼
- `patch_setup_cluster_full.sh` - setup_cluster_full.sh ì—…ë°ì´íŠ¸

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-10-11
