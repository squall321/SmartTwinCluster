# Multi-Head í´ëŸ¬ìŠ¤í„° êµ¬í˜„ ê³„íš (my_multihead_cluster.yaml ê¸°ë°˜)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### í•µì‹¬ ë³€ê²½ì‚¬í•­

**ê¸°ì¡´ ê³„íš** (cluster_config.yaml):
- ìˆ˜ë™ìœ¼ë¡œ ì‘ì„±í•œ YAML ì„¤ì • íŒŒì¼
- í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ì „ìš©

**ìƒˆ ê³„íš** (my_multihead_cluster.yaml):
- âœ… ê¸°ì¡´ `my_cluster.yaml` í¬ë§· í™•ì¥
- âœ… `controllers` ë¦¬ìŠ¤íŠ¸ë¡œ ë‹¤ì¤‘ í—¤ë“œ ì§€ì›
- âœ… ê° controllerë§ˆë‹¤ ì„œë¹„ìŠ¤ í™œì„±í™” ì˜µì…˜
  - `glusterfs`: true/false
  - `mariadb`: true/false
  - `redis`: true/false
  - `slurm`: true/false
  - `web`: true/false
  - `keepalived`: true/false
- âœ… `vip_owner` ì—¬ë¶€ ì„¤ì • (MASTER/BACKUP)
- âœ… ê¸°ì¡´ Slurm ìë™í™” ìŠ¤í¬ë¦½íŠ¸ì™€ í†µí•© ê°€ëŠ¥

### ì¥ì 

1. **ì¼ê´€ì„±**: ê¸°ì¡´ `my_cluster.yaml` ì‚¬ìš©ìì—ê²Œ ì¹œìˆ™
2. **ìœ ì—°ì„±**: ê° controllerë§ˆë‹¤ ì„œë¹„ìŠ¤ ì„ íƒ ê°€ëŠ¥
3. **í™•ì¥ì„±**: controller ì¶”ê°€ëŠ” ë¦¬ìŠ¤íŠ¸ì— í•­ëª© ì¶”ê°€ë§Œ
4. **í†µí•©ì„±**: ê¸°ì¡´ Slurm ì„¤ì¹˜ ìë™í™”ì™€ í˜¸í™˜

---

## ğŸ—ï¸ íŒŒì¼ êµ¬ì¡° (ì—…ë°ì´íŠ¸)

```
/home/koopark/claude/KooSlurmInstallAutomationRefactory/
â”œâ”€â”€ my_cluster.yaml                    # ê¸°ì¡´: ë‹¨ì¼ controller
â”œâ”€â”€ my_multihead_cluster.yaml          # âœ¨ ìƒˆë¡œ ì¶”ê°€: ë‹¤ì¤‘ controller
â”‚
â”œâ”€â”€ dashboard/                         # ì›¹ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ auth_portal_4430/
â”‚   â”œâ”€â”€ backend_5010/
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ start_complete.sh              # ê¸°ì¡´ ë‹¨ì¼ ì„œë²„ ì‹œì‘
â”‚
â””â”€â”€ cluster/                           # âœ¨ ìƒˆë¡œ ì¶”ê°€: ë©€í‹°í—¤ë“œ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
    â”œâ”€â”€ start_multihead.sh             # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ parser.py                  # my_multihead_cluster.yaml íŒŒì‹±
    â”œâ”€â”€ discovery/
    â”‚   â”œâ”€â”€ auto_discovery.sh          # í™œì„± ë…¸ë“œ íƒì§€
    â”‚   â””â”€â”€ join_cluster.sh            # í´ëŸ¬ìŠ¤í„° ì¡°ì¸
    â”œâ”€â”€ setup/
    â”‚   â”œâ”€â”€ phase0_storage.sh          # GlusterFS
    â”‚   â”œâ”€â”€ phase1_database.sh         # MariaDB Galera
    â”‚   â”œâ”€â”€ phase2_redis.sh            # Redis Cluster
    â”‚   â”œâ”€â”€ phase3_slurm.sh            # Slurm Multi-Master
    â”‚   â”œâ”€â”€ phase4_keepalived.sh       # VIP ê´€ë¦¬
    â”‚   â””â”€â”€ phase5_web.sh              # ì›¹ ì„œë¹„ìŠ¤
    â”œâ”€â”€ backup/
    â”‚   â”œâ”€â”€ backup.sh                  # ì›í´ë¦­ ë°±ì—…
    â”‚   â””â”€â”€ restore.sh                 # ì›í´ë¦­ ë³µì›
    â””â”€â”€ utils/
        â”œâ”€â”€ cluster_info.sh            # í´ëŸ¬ìŠ¤í„° ì •ë³´
        â”œâ”€â”€ node_add.sh                # ë…¸ë“œ ì¶”ê°€
        â””â”€â”€ node_remove.sh             # ë…¸ë“œ ì œê±°
```

---

## ğŸ“… Phaseë³„ êµ¬í˜„ ê³„íš (ì¬ì„¤ê³„)

---

### Phase 0: YAML íŒŒì‹± ë° ê¸°ë³¸ í”„ë ˆì„ì›Œí¬ (3ì¼)

**ëª©í‘œ**: my_multihead_cluster.yamlì„ íŒŒì‹±í•˜ê³  í™œì„± ë…¸ë“œ ì •ë³´ ì¶”ì¶œ

#### 0.1 YAML íŒŒì„œ ì‘ì„± (1ì¼)

**íŒŒì¼**: `cluster/config/parser.py`

**ê¸°ëŠ¥**:
1. my_multihead_cluster.yaml ì½ê¸°
2. í™˜ê²½ë³€ìˆ˜ ì¹˜í™˜ (`${DB_PASSWORD}` â†’ ì‹¤ì œ ê°’)
3. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
   - `get_controllers()` â†’ controllers ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
   - `get_active_controllers(service='all')` â†’ íŠ¹ì • ì„œë¹„ìŠ¤ê°€ trueì¸ controllerë§Œ ë°˜í™˜
   - `get_vip_config()` â†’ VIP ì„¤ì • ë°˜í™˜
   - `get_storage_config()` â†’ GlusterFS ì„¤ì • ë°˜í™˜
   - `get_database_config()` â†’ MariaDB ì„¤ì • ë°˜í™˜
   - `get_redis_config()` â†’ Redis ì„¤ì • ë°˜í™˜
   - `get_slurm_config()` â†’ Slurm ì„¤ì • ë°˜í™˜

**ì¶œë ¥ ì˜ˆì‹œ**:
```python
# get_active_controllers('mariadb')
[
    {
        'hostname': 'server1',
        'ip': '192.168.1.101',
        'priority': 100,
        'vip_owner': True,
        'services': {
            'glusterfs': True,
            'mariadb': True,
            'redis': True,
            'slurm': True,
            'web': True,
            'keepalived': True
        }
    },
    {
        'hostname': 'server2',
        'ip': '192.168.1.102',
        ...
    }
]
```

**ëª…ë ¹ì¤„ ë„êµ¬**:
```bash
# ëª¨ë“  controller ì¶œë ¥
./cluster/config/parser.py --list-controllers

# MariaDB í™œì„± controllerë§Œ ì¶œë ¥
./cluster/config/parser.py --service mariadb

# VIP ì„¤ì • ì¶œë ¥
./cluster/config/parser.py --get-vip

# ì„¤ì • ê²€ì¦
./cluster/config/parser.py --validate
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] parser.py ì‘ì„± ì™„ë£Œ
- [ ] my_multihead_cluster.yaml íŒŒì‹± í…ŒìŠ¤íŠ¸
- [ ] í™˜ê²½ë³€ìˆ˜ ì¹˜í™˜ í…ŒìŠ¤íŠ¸
- [ ] ê° get_* í•¨ìˆ˜ í…ŒìŠ¤íŠ¸

#### 0.2 ìë™ íƒì§€ ì‹œìŠ¤í…œ (2ì¼)

**íŒŒì¼**: `cluster/discovery/auto_discovery.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] my_multihead_cluster.yamlì—ì„œ controllers ì½ê¸°
     â†“
[2] ê° controllerì— ëŒ€í•´:
     - SSH ì ‘ì† ì‹œë„ (timeout 5ì´ˆ)
     - services.glusterfs: trueë©´ â†’ gluster peer status í™•ì¸
     - services.mariadb: trueë©´ â†’ mysql -e "SHOW STATUS LIKE 'wsrep%'" í™•ì¸
     - services.redis: trueë©´ â†’ redis-cli cluster info í™•ì¸
     - services.slurm: trueë©´ â†’ scontrol ping í™•ì¸
     - services.web: trueë©´ â†’ curl http://IP:4430/health í™•ì¸
     â†“
[3] JSON ì¶œë ¥
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```json
{
  "total_controllers": 4,
  "active_controllers": 3,
  "inactive_controllers": 1,
  "controllers": [
    {
      "hostname": "server1",
      "ip": "192.168.1.101",
      "status": "active",
      "services": {
        "glusterfs": {"status": "ok", "peers": 3},
        "mariadb": {"status": "ok", "cluster_size": 3},
        "redis": {"status": "ok", "cluster_state": "ok"},
        "slurm": {"status": "ok", "controller": "primary"},
        "web": {"status": "ok", "http_code": 200},
        "keepalived": {"status": "ok", "vip": true}
      },
      "uptime": "5 days",
      "load": 0.35
    },
    {
      "hostname": "server4",
      "ip": "192.168.1.104",
      "status": "inactive",
      "error": "Connection timeout"
    }
  ],
  "vip_owner": "server1",
  "cluster_state": "healthy"
}
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] auto_discovery.sh ì‘ì„±
- [ ] SSH ì ‘ì† í…ŒìŠ¤íŠ¸
- [ ] ê° ì„œë¹„ìŠ¤ë³„ ìƒíƒœ í™•ì¸ í…ŒìŠ¤íŠ¸
- [ ] JSON ì¶œë ¥ ê²€ì¦

**Phase 0 ì™„ë£Œ ì¡°ê±´**:
- âœ… my_multihead_cluster.yaml íŒŒì‹± ê°€ëŠ¥
- âœ… í™œì„± controller ìë™ íƒì§€ ê°€ëŠ¥
- âœ… ê° ì„œë¹„ìŠ¤ë³„ ìƒíƒœ í™•ì¸ ê°€ëŠ¥

---

### Phase 1: GlusterFS ë™ì  í´ëŸ¬ìŠ¤í„°ë§ (1ì£¼)

**ëª©í‘œ**: services.glusterfs: trueì¸ controllerë“¤ë¡œ GlusterFS í´ëŸ¬ìŠ¤í„° êµ¬ì„±

#### 1.1 GlusterFS ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ (3ì¼)

**íŒŒì¼**: `cluster/setup/phase0_storage.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ GlusterFS ì„¤ì • ì½ê¸°
     - glusterfs í™œì„± controller ë¦¬ìŠ¤íŠ¸
     - volume_name, replica_count, brick_path ë“±
     â†“
[2] í˜„ì¬ ì„œë²„ê°€ glusterfs: trueì¸ì§€ í™•ì¸
     - falseë©´ â†’ ìŠ¤í‚µ
     â†“
[3] GlusterFS ì„¤ì¹˜ í™•ì¸
     - ë¯¸ì„¤ì¹˜ ì‹œ ìë™ ì„¤ì¹˜
     â†“
[4] í™œì„± GlusterFS ë…¸ë“œ íƒì§€
     - auto_discovery.sh í˜¸ì¶œ
     - glusterfs ìƒíƒœê°€ 'ok'ì¸ ë…¸ë“œ ì¶”ì¶œ
     â†“
[5] Peer ì—°ê²° ì—¬ë¶€ í™•ì¸
     - ì´ë¯¸ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´ â†’ ìŠ¤í‚µ
     - ì—°ê²° ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ â†’ gluster peer probe
     â†“
[6] Volume ìƒì„± ë˜ëŠ” ì¡°ì¸
     - í™œì„± ë…¸ë“œ 0ê°œ â†’ ìƒˆ volume ìƒì„± (Bootstrap)
     - í™œì„± ë…¸ë“œ 1ê°œ+ â†’ ê¸°ì¡´ volumeì— brick ì¶”ê°€
     â†“
[7] Volume ë§ˆìš´íŠ¸
     - mkdir -p /mnt/gluster
     - mount -t glusterfs localhost:/<volume_name> /mnt/gluster
     - /etc/fstabì— ë“±ë¡
     â†“
[8] ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (Bootstrapë§Œ)
     - frontend_builds/, slurm/, uploads/, config/
     â†“
[9] ê²€ì¦
     - gluster volume status
     - df -h | grep gluster
```

**ì˜µì…˜**:
```bash
./cluster/setup/phase0_storage.sh
  --config /path/to/my_multihead_cluster.yaml  # ì„¤ì • íŒŒì¼ ê²½ë¡œ
  --bootstrap                                    # ê°•ì œ Bootstrap
  --join                                         # ê°•ì œ Join
  --dry-run                                      # ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì ˆì°¨ë§Œ ì¶œë ¥
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] phase0_storage.sh ì‘ì„±
- [ ] GlusterFS ì„¤ì¹˜ ìë™í™”
- [ ] Peer ì—°ê²° í…ŒìŠ¤íŠ¸
- [ ] Volume ìƒì„±/ì¡°ì¸ í…ŒìŠ¤íŠ¸
- [ ] ë§ˆìš´íŠ¸ ë° fstab ë“±ë¡ í…ŒìŠ¤íŠ¸

#### 1.2 Brick ë™ì  ì¶”ê°€/ì œê±° (2ì¼)

**íŒŒì¼**: `cluster/utils/node_add.sh`, `cluster/utils/node_remove.sh`

**ë…¸ë“œ ì¶”ê°€** (node_add.sh):
```bash
./cluster/utils/node_add.sh --service glusterfs --ip 192.168.1.105

ì‹¤í–‰ íë¦„:
[1] ìƒˆ ë…¸ë“œì˜ services.glusterfs: true í™•ì¸
[2] Peer probe
[3] Volumeì— brick ì¶”ê°€
[4] Volume rebalance (ë°ì´í„° ì¬ë¶„ë°°)
```

**ë…¸ë“œ ì œê±°** (node_remove.sh):
```bash
./cluster/utils/node_remove.sh --service glusterfs --ip 192.168.1.104

ì‹¤í–‰ íë¦„:
[1] Volumeì—ì„œ brick ì œê±°
[2] Volume rebalance
[3] Peer detach
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] node_add.sh ì‘ì„±
- [ ] node_remove.sh ì‘ì„±
- [ ] Brick ì¶”ê°€/ì œê±° ë¬´ì¤‘ë‹¨ í…ŒìŠ¤íŠ¸

#### 1.3 ë°±ì—… ì €ì¥ì†Œ ì¤€ë¹„ (2ì¼)

**ë””ë ‰í† ë¦¬**: `/data/system_backup` (ê° controller ë¡œì»¬ ë””ìŠ¤í¬)

**êµ¬ì¡°**:
```
/data/system_backup/
â”œâ”€â”€ configs/
â”œâ”€â”€ databases/
â”œâ”€â”€ state/
â””â”€â”€ snapshots/
    â””â”€â”€ YYYYMMDD_HHMMSS/
        â”œâ”€â”€ metadata.json
        â”œâ”€â”€ configs.tar.gz
        â”œâ”€â”€ mariadb_dump.sql.gz
        â””â”€â”€ redis_dump.rdb
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸

**Phase 1 ì™„ë£Œ ì¡°ê±´**:
- âœ… GlusterFS N-node í´ëŸ¬ìŠ¤í„° ì‘ë™
- âœ… ë™ì  ë…¸ë“œ ì¶”ê°€/ì œê±° ê°€ëŠ¥
- âœ… ë°±ì—… ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ

---

### Phase 2: MariaDB Galera ë™ì  í´ëŸ¬ìŠ¤í„°ë§ (1ì£¼)

**ëª©í‘œ**: services.mariadb: trueì¸ controllerë“¤ë¡œ Galera í´ëŸ¬ìŠ¤í„° êµ¬ì„±

#### 2.1 MariaDB Galera ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ (3ì¼)

**íŒŒì¼**: `cluster/setup/phase1_database.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ MariaDB ì„¤ì • ì½ê¸°
     - mariadb í™œì„± controller ë¦¬ìŠ¤íŠ¸
     - database.mariadb.databases ë¦¬ìŠ¤íŠ¸
     â†“
[2] í˜„ì¬ ì„œë²„ê°€ mariadb: trueì¸ì§€ í™•ì¸
     - falseë©´ â†’ ìŠ¤í‚µ
     â†“
[3] MariaDB + Galera ì„¤ì¹˜
     â†“
[4] í™œì„± Galera ë…¸ë“œ íƒì§€
     - auto_discovery.sh í˜¸ì¶œ
     â†“
[5] galera.cnf ë™ì  ìƒì„±
     - wsrep_cluster_addressì— ëª¨ë“  í™œì„± ë…¸ë“œ IP ì…ë ¥
     â†“
[6] Bootstrap vs Join ê²°ì •
     - í™œì„± Galera ë…¸ë“œ 0ê°œ â†’ Bootstrap
     - í™œì„± Galera ë…¸ë“œ 1ê°œ+ â†’ Join
     â†“
[7] ì‹œì‘
     - Bootstrap: galera_new_cluster
     - Join: systemctl start mariadb (ìë™ SST)
     â†“
[8] ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (Bootstrapë§Œ)
     - CREATE DATABASE slurm_acct_db
     - CREATE DATABASE auth_portal
     - CREATE USER ...
     â†“
[9] ê²€ì¦
     - SHOW STATUS LIKE 'wsrep_cluster_size'
     - SHOW STATUS LIKE 'wsrep_local_state_comment'
```

**galera.cnf í…œí”Œë¦¿**:
```ini
# /etc/mysql/mariadb.conf.d/galera.cnf.template

[galera]
wsrep_on=ON
wsrep_provider=/usr/lib/galera/libgalera_smm.so
wsrep_cluster_name="{{cluster_name}}"
wsrep_cluster_address="gcomm://{{cluster_addresses}}"  # íŒŒì‹± ê²°ê³¼ ì…ë ¥
wsrep_node_address="{{node_ip}}"
wsrep_node_name="{{node_name}}"
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
wsrep_sst_method={{sst_method}}
wsrep_slave_threads={{slave_threads}}
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] phase1_database.sh ì‘ì„±
- [ ] MariaDB + Galera ì„¤ì¹˜ ìë™í™”
- [ ] galera.cnf ë™ì  ìƒì„± í…ŒìŠ¤íŠ¸
- [ ] Bootstrap ëª¨ë“œ í…ŒìŠ¤íŠ¸
- [ ] Join ëª¨ë“œ í…ŒìŠ¤íŠ¸ (SST í™•ì¸)
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸

#### 2.2 ë™ì  ë…¸ë“œ ì¶”ê°€/ì œê±° (2ì¼)

**ë…¸ë“œ ì¶”ê°€**:
```bash
./cluster/utils/node_add.sh --service mariadb --ip 192.168.1.105

ì‹¤í–‰:
[1] ìƒˆ ë…¸ë“œì˜ galera.cnf ìƒì„± (ëª¨ë“  í™œì„± ë…¸ë“œ í¬í•¨)
[2] systemctl start mariadb (ìë™ SSTë¡œ ë°ì´í„° ë™ê¸°í™”)
[3] ê¸°ì¡´ ë…¸ë“œë“¤ì˜ galera.cnf ì—…ë°ì´íŠ¸ (ìƒˆ IP ì¶”ê°€)
[4] SET GLOBAL wsrep_cluster_address='gcomm://...' (ì¬ì‹œì‘ ë¶ˆí•„ìš”!)
```

**ë…¸ë“œ ì œê±°**:
```bash
./cluster/utils/node_remove.sh --service mariadb --ip 192.168.1.104

ì‹¤í–‰:
[1] ì œê±° ë…¸ë“œì—ì„œ MariaDB ì¤‘ì§€
[2] ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì˜ galera.cnf ì—…ë°ì´íŠ¸ (ì œê±° IP ì‚­ì œ)
[3] SET GLOBAL wsrep_cluster_address='gcomm://...'
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë…¸ë“œ ì¶”ê°€ ë¬´ì¤‘ë‹¨ í…ŒìŠ¤íŠ¸
- [ ] ë…¸ë“œ ì œê±° ë¬´ì¤‘ë‹¨ í…ŒìŠ¤íŠ¸

#### 2.3 ë°±ì—… ë° ë³µì› (2ì¼)

**ë°±ì—… ìŠ¤í¬ë¦½íŠ¸**: `cluster/backup/backup_mariadb.sh`

```bash
mysqldump --all-databases --single-transaction | gzip > /data/system_backup/databases/mariadb_$(date +%Y%m%d_%H%M%S).sql.gz
```

**ë³µì› ìŠ¤í¬ë¦½íŠ¸**: `cluster/backup/restore_mariadb.sh`

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ë³µì› ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ë°±ì—…/ë³µì› í†µí•© í…ŒìŠ¤íŠ¸

**Phase 2 ì™„ë£Œ ì¡°ê±´**:
- âœ… MariaDB Galera N-node í´ëŸ¬ìŠ¤í„° ì‘ë™
- âœ… ë™ì  ë…¸ë“œ ì¶”ê°€/ì œê±° ê°€ëŠ¥
- âœ… ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ ì‘ë™

---

### Phase 3: Redis Cluster ë™ì  í´ëŸ¬ìŠ¤í„°ë§ (1ì£¼)

**ëª©í‘œ**: services.redis: trueì¸ controllerë“¤ë¡œ Redis Cluster êµ¬ì„±

#### 3.1 Redis Cluster ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ (3ì¼)

**íŒŒì¼**: `cluster/setup/phase2_redis.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ Redis ì„¤ì • ì½ê¸°
     - redis í™œì„± controller ë¦¬ìŠ¤íŠ¸
     - redis.cluster.password ë“±
     â†“
[2] í˜„ì¬ ì„œë²„ê°€ redis: trueì¸ì§€ í™•ì¸
     â†“
[3] Redis ì„¤ì¹˜
     â†“
[4] redis.conf ë™ì  ìƒì„±
     - cluster-enabled yes
     - requirepass {{password}}
     â†“
[5] Redis ì‹œì‘
     â†“
[6] í™œì„± Redis ë…¸ë“œ íƒì§€
     â†“
[7] Cluster ìƒì„± vs ì¡°ì¸
     - í™œì„± ë…¸ë“œ 0-2ê°œ â†’ ëŒ€ê¸° (ìµœì†Œ 3ê°œ í•„ìš”)
     - í™œì„± ë…¸ë“œ 3ê°œ+ â†’ Cluster ìƒì„± ë˜ëŠ” ì¡°ì¸
     â†“
[8] Cluster ëª…ë ¹ ì‹¤í–‰
     - ìƒì„±: redis-cli --cluster create ...
     - ì¡°ì¸: redis-cli --cluster add-node ...
     â†“
[9] í•´ì‹œ ìŠ¬ë¡¯ ì¬ë¶„ë°° (ì¡°ì¸ ì‹œ)
     - redis-cli --cluster rebalance
```

**íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: 2ê°œ ë…¸ë“œ**
- Redis ClusterëŠ” ìµœì†Œ 3ê°œ í•„ìš”
- ëŒ€ì•ˆ: Redis Sentinel (Master-Replica)

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] phase2_redis.sh ì‘ì„±
- [ ] Redis Cluster ìƒì„± í…ŒìŠ¤íŠ¸ (3ê°œ+)
- [ ] ë…¸ë“œ ì¡°ì¸ í…ŒìŠ¤íŠ¸
- [ ] í•´ì‹œ ìŠ¬ë¡¯ ì¬ë¶„ë°° í…ŒìŠ¤íŠ¸
- [ ] 2ê°œ ë…¸ë“œ Sentinel ëª¨ë“œ êµ¬í˜„ (ì„ íƒ)

#### 3.2 ë™ì  ë…¸ë“œ ì¶”ê°€/ì œê±° (2ì¼)

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë…¸ë“œ ì¶”ê°€ í…ŒìŠ¤íŠ¸
- [ ] ë…¸ë“œ ì œê±° í…ŒìŠ¤íŠ¸ (ìŠ¬ë¡¯ ì´ë™ í™•ì¸)

#### 3.3 ë°±ì—… ë° ë³µì› (2ì¼)

**ë°±ì—…**: RDB ìŠ¤ëƒ…ìƒ· ë³µì‚¬

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ë³µì› ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

**Phase 3 ì™„ë£Œ ì¡°ê±´**:
- âœ… Redis Cluster N-node ì‘ë™
- âœ… ë™ì  ë…¸ë“œ ì¶”ê°€/ì œê±° ê°€ëŠ¥

---

### Phase 4: Slurm Multi-Master ë™ì  êµ¬ì„± (1ì£¼)

**ëª©í‘œ**: services.slurm: trueì¸ controllerë“¤ë¡œ Slurm Multi-Master êµ¬ì„±

#### 4.1 Slurm ì„¤ì • ë™ì  ìƒì„± (3ì¼)

**íŒŒì¼**: `cluster/setup/phase3_slurm.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ Slurm ì„¤ì • ì½ê¸°
     - slurm í™œì„± controller ë¦¬ìŠ¤íŠ¸
     â†“
[2] í˜„ì¬ ì„œë²„ê°€ slurm: trueì¸ì§€ í™•ì¸
     â†“
[3] Slurm ì„¤ì¹˜ (ê¸°ì¡´ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ í™œìš©)
     â†“
[4] slurm.conf ë™ì  ìƒì„±
     - ëª¨ë“  í™œì„± controllerë¥¼ SlurmctldHostë¡œ ë“±ë¡
     - StateSaveLocation=/mnt/gluster/slurm/state
     â†“
[5] slurm.confë¥¼ GlusterFSì— ì €ì¥
     - /mnt/gluster/slurm/config/slurm.conf
     â†“
[6] ì‹¬ë³¼ë¦­ ë§í¬
     - ln -s /mnt/gluster/slurm/config/slurm.conf /etc/slurm/slurm.conf
     â†“
[7] slurmdbd ì„¤ì •
     - StorageHost=127.0.0.1 (ë¡œì»¬ Galera ë…¸ë“œ)
     â†“
[8] ì„œë¹„ìŠ¤ ì‹œì‘
     - slurmdbd ë¨¼ì €
     - slurmctld
```

**slurm.conf í…œí”Œë¦¿**:
```bash
# ë™ì  ìƒì„± ë¶€ë¶„
{{#each controllers}}
SlurmctldHost={{hostname}}({{ip}})
{{/each}}

StateSaveLocation=/mnt/gluster/slurm/state
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldLogFile=/mnt/gluster/slurm/logs/slurmctld.log

AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=127.0.0.1
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] phase3_slurm.sh ì‘ì„±
- [ ] slurm.conf ë™ì  ìƒì„± í…ŒìŠ¤íŠ¸
- [ ] Multi-Master ì„¤ì • í™•ì¸ (scontrol show config)

#### 4.2 VIP ì „í™˜ ì‹œ ìë™ Controller ì „í™˜ (2ì¼)

**Keepalived ì•Œë¦¼ ìŠ¤í¬ë¦½íŠ¸**:
```bash
# /usr/local/bin/notify_master.sh
systemctl restart slurmctld
# VIP ë°›ìœ¼ë©´ Primary Controller

# /usr/local/bin/notify_backup.sh
systemctl restart slurmctld
# VIP ìƒìœ¼ë©´ Backup Controller
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì•Œë¦¼ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] VIP ì „í™˜ í…ŒìŠ¤íŠ¸

#### 4.3 ë™ì  ë…¸ë“œ ì¶”ê°€/ì œê±° (2ì¼)

**ë…¸ë“œ ì¶”ê°€**:
```
[1] slurm.confì— SlurmctldHost ì¶”ê°€
[2] GlusterFSì— ì €ì¥ (ìë™ ë™ê¸°í™”)
[3] scontrol reconfigure (ëª¨ë“  ë…¸ë“œ)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë…¸ë“œ ì¶”ê°€ ë¬´ì¤‘ë‹¨ í…ŒìŠ¤íŠ¸

**Phase 4 ì™„ë£Œ ì¡°ê±´**:
- âœ… Slurm Multi-Master ì‘ë™
- âœ… VIP ì „í™˜ ì‹œ ìë™ Controller ì „í™˜

---

### Phase 5: Keepalived VIP ë™ì  ê´€ë¦¬ (3ì¼)

**ëª©í‘œ**: services.keepalived: trueì¸ controllerë“¤ë¡œ VIP ê´€ë¦¬

#### 5.1 Keepalived ì„¤ì • ë™ì  ìƒì„± (2ì¼)

**íŒŒì¼**: `cluster/setup/phase4_keepalived.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ VIP ì„¤ì • ì½ê¸°
     - network.vip.address
     - network.vip.interface
     - í˜„ì¬ controllerì˜ priority
     - vip_owner ì—¬ë¶€
     â†“
[2] í˜„ì¬ ì„œë²„ê°€ keepalived: trueì¸ì§€ í™•ì¸
     â†“
[3] keepalived.conf ë™ì  ìƒì„±
     - state: vip_owner=true â†’ MASTER, false â†’ BACKUP
     - priority: YAMLì—ì„œ ì½ì–´ì˜´
     â†“
[4] Keepalived ì‹œì‘
     â†“
[5] VIP í™•ì¸
     - MASTERì¸ ê²½ìš° VIP ì†Œìœ  í™•ì¸
```

**keepalived.conf í…œí”Œë¦¿**:
```
vrrp_instance VI_1 {
    state {{state}}              # MASTER or BACKUP
    interface {{interface}}
    virtual_router_id {{vrrp_id}}
    priority {{priority}}
    ...
    virtual_ipaddress {
        {{vip_address}}
    }
    track_script {
        check_all_services
    }
}
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] phase4_keepalived.sh ì‘ì„±
- [ ] keepalived.conf ë™ì  ìƒì„± í…ŒìŠ¤íŠ¸
- [ ] VIP í• ë‹¹ í™•ì¸

#### 5.2 í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ (1ì¼)

**íŒŒì¼**: `/usr/local/bin/check_all_services.sh`

**ì²´í¬ í•­ëª©** (YAML ê¸°ë°˜):
```bash
# services.glusterfs: trueë©´ â†’ gluster volume status í™•ì¸
# services.mariadb: trueë©´ â†’ mysql -e "SHOW STATUS LIKE 'wsrep_ready'" í™•ì¸
# services.redis: trueë©´ â†’ redis-cli ping í™•ì¸
# services.slurm: trueë©´ â†’ scontrol ping í™•ì¸
# services.web: trueë©´ â†’ curl http://localhost:4430/health í™•ì¸
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] check_all_services.sh ì‘ì„±
- [ ] YAML ê¸°ë°˜ ë™ì  ì²´í¬ í…ŒìŠ¤íŠ¸

**Phase 5 ì™„ë£Œ ì¡°ê±´**:
- âœ… Keepalived VIP ìë™ ê´€ë¦¬
- âœ… í—¬ìŠ¤ì²´í¬ ê¸°ë°˜ ìë™ ì „í™˜

---

### Phase 6: ì›¹ ì„œë¹„ìŠ¤ í†µí•© (1ì£¼)

**ëª©í‘œ**: services.web: trueì¸ controllerë“¤ì— ì›¹ ì„œë¹„ìŠ¤ ë°°í¬

#### 6.1 ì›¹ ì„œë¹„ìŠ¤ ìë™ ë°°í¬ (3ì¼)

**íŒŒì¼**: `cluster/setup/phase5_web.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ ì›¹ ì„œë¹„ìŠ¤ ì„¤ì • ì½ê¸°
     - web_services.services ë¦¬ìŠ¤íŠ¸
     â†“
[2] í˜„ì¬ ì„œë²„ê°€ web: trueì¸ì§€ í™•ì¸
     â†“
[3] ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ë°°í¬
     - Python venv ìƒì„±
     - pip install -r requirements.txt
     - í™˜ê²½ë³€ìˆ˜ ì„¤ì • (GlusterFS ê³µìœ )
     - Systemd ì„œë¹„ìŠ¤ ìƒì„±
     â†“
[4] í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ (1ê°œ ì„œë²„ì—ì„œë§Œ)
     - npm install
     - npm run build
     - /mnt/gluster/frontend_builds/ë¡œ ë³µì‚¬
     â†“
[5] Nginx ì„¤ì •
     - ì •ì  íŒŒì¼ ê²½ë¡œ: /mnt/gluster/frontend_builds/
     - ë°±ì—”ë“œ í”„ë¡ì‹œ: localhost
     â†“
[6] ì„œë¹„ìŠ¤ ì‹œì‘
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] phase5_web.sh ì‘ì„±
- [ ] ë°±ì—”ë“œ ë°°í¬ í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ í…ŒìŠ¤íŠ¸
- [ ] Nginx ì„¤ì • í…ŒìŠ¤íŠ¸

#### 6.2 Redis/DB ì—°ê²° ìˆ˜ì • (2ì¼)

**Redis Cluster ì—°ê²°** (Python):
```python
# cluster/config/parser.pyë¥¼ í™œìš©
redis_nodes = get_active_controllers('redis')
redis_client = RedisCluster(
    startup_nodes=[{"host": node['ip'], "port": 6379} for node in redis_nodes],
    password=os.getenv('REDIS_PASSWORD')
)
```

**MariaDB ì—°ê²°**:
```python
# ë¡œì»¬ Galera ë…¸ë“œ ì—°ê²°
db = MySQLdb.connect(host='127.0.0.1', user='...', password='...')
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Redis Cluster ì—°ê²° ìˆ˜ì •
- [ ] MariaDB ì—°ê²° í™•ì¸

#### 6.3 Health Check ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (2ì¼)

**ëª¨ë“  ë°±ì—”ë“œì— `/health` ì¶”ê°€**

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] /health ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (ëª¨ë“  ë°±ì—”ë“œ)
- [ ] Keepalived ì—°ë™ í…ŒìŠ¤íŠ¸

**Phase 6 ì™„ë£Œ ì¡°ê±´**:
- âœ… ì›¹ ì„œë¹„ìŠ¤ ëª¨ë“  controllerì— ë°°í¬
- âœ… Redis/DB í´ëŸ¬ìŠ¤í„° ì—°ê²°
- âœ… Health Check ì‘ë™

---

### Phase 7: í†µí•© ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ (1ì£¼)

**ëª©í‘œ**: ì›í´ë¦­ ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ êµ¬ì¶•

#### 7.1 í†µí•© ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ (3ì¼)

**íŒŒì¼**: `cluster/backup/backup.sh`

**ì‹¤í–‰ íë¦„**:
```
[1] YAMLì—ì„œ ë°±ì—… ì„¤ì • ì½ê¸°
     - shared_storage.backup.items
     - shared_storage.backup.retention
     â†“
[2] ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
     - /data/system_backup/snapshots/YYYYMMDD_HHMMSS/
     â†“
[3] ë°±ì—… ì‹¤í–‰
     - configs: /etc/nginx, /etc/redis, /etc/mysql, /etc/slurm
     - databases: mysqldump, redis SAVE
     - slurm_state: /mnt/gluster/slurm/state
     - glusterfs_meta: gluster volume info
     â†“
[4] metadata.json ìƒì„±
     - ë°±ì—… ID, ì‹œê°„, í˜¸ìŠ¤íŠ¸, í´ëŸ¬ìŠ¤í„° í¬ê¸°, ì†Œí”„íŠ¸ì›¨ì–´ ë²„ì „
     â†“
[5] ì••ì¶•
     - tar + gzip
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] backup.sh ì‘ì„±
- [ ] Full Backup í…ŒìŠ¤íŠ¸
- [ ] metadata.json ìƒì„± í™•ì¸

#### 7.2 í†µí•© ë³µì› ìŠ¤í¬ë¦½íŠ¸ (2ì¼)

**íŒŒì¼**: `cluster/backup/restore.sh`

**ì˜µì…˜**:
```bash
./cluster/backup/restore.sh --latest
./cluster/backup/restore.sh --id 20251026_140530
./cluster/backup/restore.sh --only-configs
./cluster/backup/restore.sh --dry-run
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] restore.sh ì‘ì„±
- [ ] ì „ì²´ ë³µì› í…ŒìŠ¤íŠ¸
- [ ] Selective Restore í…ŒìŠ¤íŠ¸

#### 7.3 ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ (2ì¼)

**íŒŒì¼**: `cluster/backup/backup_scheduler.sh`

**ê¸°ëŠ¥**:
- YAMLì—ì„œ ë°±ì—… ìŠ¤ì¼€ì¤„ ì½ê¸°
- Cron ì‘ì—… ìë™ ë“±ë¡
- ë°±ì—… ë³´ì¡´ ì •ì±… ì ìš©

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] backup_scheduler.sh ì‘ì„±
- [ ] Cron ë“±ë¡ í…ŒìŠ¤íŠ¸
- [ ] ë³´ì¡´ ì •ì±… í…ŒìŠ¤íŠ¸

**Phase 7 ì™„ë£Œ ì¡°ê±´**:
- âœ… ì›í´ë¦­ ë°±ì—… ì‹œìŠ¤í…œ
- âœ… ì›í´ë¦­ ë³µì› ì‹œìŠ¤í…œ
- âœ… ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬

---

### Phase 8: ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (1ì£¼)

**ëª©í‘œ**: start_multihead.sh í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

#### 8.1 start_multihead.sh ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ (5ì¼)

**íŒŒì¼**: `cluster/start_multihead.sh`

**ì˜µì…˜**:
```bash
./cluster/start_multihead.sh
  --config my_multihead_cluster.yaml  # ì„¤ì • íŒŒì¼ (ê¸°ë³¸ê°’: ../my_multihead_cluster.yaml)
  --bootstrap                          # ê°•ì œ Bootstrap (ìƒˆ í´ëŸ¬ìŠ¤í„°)
  --join                               # ê°•ì œ Join (ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°)
  --standalone                         # ë‹¨ë… ëª¨ë“œ
  --status                             # í˜„ì¬ ìƒíƒœë§Œ í™•ì¸
  --force                              # í™•ì¸ í”„ë¡¬í”„íŠ¸ ìŠ¤í‚µ
  --dry-run                            # ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì ˆì°¨ë§Œ ì¶œë ¥
  --phase <phase_number>               # íŠ¹ì • Phaseë§Œ ì‹¤í–‰
```

**ì‹¤í–‰ íë¦„**:
```
[1] í™˜ê²½ ì²´í¬
     - root ê¶Œí•œ í™•ì¸
     - my_multihead_cluster.yaml ì¡´ì¬ í™•ì¸
     - /data, /mnt/gluster í™•ì¸
     â†“
[2] YAML íŒŒì‹±
     - í˜„ì¬ ì„œë²„ ì •ë³´ í™•ì¸ (IP ê¸°ë°˜)
     - í™œì„±í™”ëœ ì„œë¹„ìŠ¤ í™•ì¸
     â†“
[3] ëª¨ë“œ ê²°ì •
     - --bootstrap â†’ Bootstrap
     - --standalone â†’ Standalone
     - --join â†’ Join
     - ê¸°ë³¸ê°’ â†’ ìë™ íƒì§€
     â†“
[4] ìë™ íƒì§€ (ê¸°ë³¸ê°’)
     - auto_discovery.sh ì‹¤í–‰
     - í™œì„± controller ìˆ˜ í™•ì¸
     - 0ê°œ â†’ Bootstrap
     - 1ê°œ+ â†’ Join
     â†“
[5] Phaseë³„ ì‹¤í–‰
     - Phase 0: GlusterFS (services.glusterfs: trueì¸ ê²½ìš°)
     - Phase 1: MariaDB (services.mariadb: true)
     - Phase 2: Redis (services.redis: true)
     - Phase 3: Slurm (services.slurm: true)
     - Phase 4: Keepalived (services.keepalived: true)
     - Phase 5: Web (services.web: true)
     â†“
[6] í—¬ìŠ¤ì²´í¬
     - 5ì´ˆ ëŒ€ê¸° í›„ ëª¨ë“  ì„œë¹„ìŠ¤ í™•ì¸
     â†“
[7] í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶œë ¥
     - cluster_info.sh í˜¸ì¶œ
     â†“
[8] ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ì„ íƒ)
```

**ì—ëŸ¬ ì²˜ë¦¬**:
- ê° Phase ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
- ë¡œê·¸ íŒŒì¼: `/var/log/cluster_multihead.log`

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] start_multihead.sh ì‘ì„±
- [ ] Bootstrap ëª¨ë“œ í…ŒìŠ¤íŠ¸
- [ ] Join ëª¨ë“œ í…ŒìŠ¤íŠ¸
- [ ] Standalone ëª¨ë“œ í…ŒìŠ¤íŠ¸
- [ ] ìë™ íƒì§€ í…ŒìŠ¤íŠ¸
- [ ] Phaseë³„ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡¤ë°± í…ŒìŠ¤íŠ¸

#### 8.2 cluster_info.sh ì •ë³´ ì¶œë ¥ ìŠ¤í¬ë¦½íŠ¸ (2ì¼)

**íŒŒì¼**: `cluster/utils/cluster_info.sh`

**ì¶œë ¥ ì˜ˆì‹œ**:
```
========================================
HPC Portal Multi-Head Cluster Status
========================================
Cluster Name: hpc-portal-multihead
Cluster Size: 4-node (Quad Redundancy)
VIP: 192.168.1.100 (owned by server1)
Uptime: 15 days

Controller Status:
+------------+---------------+----------+--------+-----+-----+-----+-----+-----+
| Hostname   | IP            | Status   | Load   | GFS | MDB | RDS | SLM | WEB |
+------------+---------------+----------+--------+-----+-----+-----+-----+-----+
| server1    | 192.168.1.101 | Active   | 0.35   | âœ“   | âœ“   | âœ“   | âœ“   | âœ“   |
| server2    | 192.168.1.102 | Active   | 0.42   | âœ“   | âœ“   | âœ“   | âœ“   | âœ“   |
| server3    | 192.168.1.103 | Active   | 0.38   | âœ“   | âœ“   | âœ“   | âœ“   | âœ“   |
| server4    | 192.168.1.104 | Active   | 0.29   | âœ“   | âœ“   | âœ“   | âœ“   | âœ“   |
+------------+---------------+----------+--------+-----+-----+-----+-----+-----+
GFS=GlusterFS, MDB=MariaDB, RDS=Redis, SLM=Slurm, WEB=Web

Service Details:
- GlusterFS: 4/4 bricks (Replica 4, Healthy)
- MariaDB Galera: 4/4 nodes (Synced)
- Redis Cluster: 4/4 masters (cluster_state: ok)
- Slurm: Primary on server1 (3 backups ready)
- Web Services: 8 services Ã— 4 nodes = 32 instances

Storage:
- Shared (GlusterFS): 850GB / 2TB (42%)
- Backup (Local): 120GB / 500GB (24%)

Last Backup: 2025-10-26 02:00:00
========================================
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] cluster_info.sh ì‘ì„±
- [ ] í‘œ í˜•ì‹ ì¶œë ¥ í…ŒìŠ¤íŠ¸

**Phase 8 ì™„ë£Œ ì¡°ê±´**:
- âœ… start_multihead.sh ì •ìƒ ì‘ë™
- âœ… cluster_info.sh ì •ë³´ ì¶œë ¥

---

### Phase 9: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (2ì£¼)

**ëª©í‘œ**: ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

#### 9.1 ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (1ì£¼)

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:

1. **ì‹ ê·œ í´ëŸ¬ìŠ¤í„° ìƒì„±**
   ```bash
   # Server1
   ./cluster/start_multihead.sh --config my_multihead_cluster.yaml --bootstrap
   â†’ âœ… 1ì¤‘í™” í´ëŸ¬ìŠ¤í„° ìƒì„±
   ```

2. **ë…¸ë“œ ì¡°ì¸ (2ì¤‘í™”)**
   ```bash
   # Server2
   ./cluster/start_multihead.sh --config my_multihead_cluster.yaml
   â†’ ìë™ìœ¼ë¡œ Server1 íƒì§€
   â†’ âœ… 1ì¤‘í™” â†’ 2ì¤‘í™” ì™„ë£Œ
   ```

3. **ë…¸ë“œ ê³„ì† ì¡°ì¸ (3ì¤‘í™”, 4ì¤‘í™”)**
   ```bash
   # Server3
   ./cluster/start_multihead.sh
   â†’ âœ… 2ì¤‘í™” â†’ 3ì¤‘í™”

   # Server4
   ./cluster/start_multihead.sh
   â†’ âœ… 3ì¤‘í™” â†’ 4ì¤‘í™”
   ```

4. **ì„œë¹„ìŠ¤ë³„ í™œì„±í™” í…ŒìŠ¤íŠ¸**
   ```yaml
   # my_multihead_cluster.yamlì—ì„œ server3ì˜ services.redis: falseë¡œ ë³€ê²½
   # Server3 ì¬ì‹œì‘
   â†’ RedisëŠ” 3-node í´ëŸ¬ìŠ¤í„°ë¡œ ì‘ë™
   ```

5. **ì›¹ UI ì ‘ì†**
   ```
   https://192.168.1.100/  (VIP)
   â†’ SSO ë¡œê·¸ì¸
   â†’ Dashboard, CAE, App ëª¨ë‘ ì‘ë™ í™•ì¸
   ```

6. **Slurm ì‘ì—… ì œì¶œ**
   ```bash
   sbatch test_job.sh
   â†’ ì‘ì—… ì •ìƒ ì œì¶œ ë° ì‹¤í–‰ í™•ì¸
   ```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ëª¨ë“  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼

#### 9.2 ì¥ì•  í…ŒìŠ¤íŠ¸ (1ì£¼)

**ì‹œë‚˜ë¦¬ì˜¤**:

1. **ë‹¨ì¼ ë…¸ë“œ ë‹¤ìš´**
   ```bash
   # Server1 ì¤‘ì§€
   systemctl poweroff
   â†’ VIPê°€ Server2ë¡œ ì´ë™ (2-3ì´ˆ)
   â†’ ì›¹ì„œë¹„ìŠ¤ ê³„ì† ì‘ë™
   â†’ Slurm ì‘ì—… ì˜í–¥ ì—†ìŒ
   ```

2. **2ê°œ ë…¸ë“œ ë‹¤ìš´**
   ```bash
   # Server1, Server2 ì¤‘ì§€
   â†’ Server3ì´ VIP ì†Œìœ 
   â†’ 2-node í´ëŸ¬ìŠ¤í„°ë¡œ ê³„ì† ì‘ë™
   ```

3. **ë°±ì—… ë° ë³µì›**
   ```bash
   # ë°±ì—…
   ./cluster/backup/backup.sh

   # ë°ì´í„° ì‚­ì œ ì‹œë®¬ë ˆì´ì…˜
   mysql -e "DROP DATABASE auth_portal;"

   # ë³µì›
   ./cluster/backup/restore.sh --latest
   â†’ ë°ì´í„° ë³µêµ¬ í™•ì¸
   ```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ëª¨ë“  ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ í†µê³¼

**Phase 9 ì™„ë£Œ ì¡°ê±´**:
- âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í†µê³¼

---

### Phase 10: ë¬¸ì„œí™” ë° ë°°í¬ (1ì£¼)

**ëª©í‘œ**: ì‚¬ìš©ì ë§¤ë‰´ì–¼ ì‘ì„±

#### 10.1 ë¬¸ì„œ ì‘ì„± (5ì¼)

**ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸**:

1. **MULTIHEAD_QUICKSTART.md**
   - my_multihead_cluster.yaml ì‘ì„± ë°©ë²•
   - ì²« ì„œë²„ ì‹œì‘ (Bootstrap)
   - ì¶”ê°€ ì„œë²„ ì¡°ì¸
   - í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸

2. **MULTIHEAD_CONFIG_REFERENCE.md**
   - my_multihead_cluster.yaml ì „ì²´ ìŠ¤í™
   - ê° ì„¹ì…˜ ì„¤ëª…
   - services ì˜µì…˜ ì„¤ëª…

3. **MULTIHEAD_OPERATIONS.md**
   - ë…¸ë“œ ì¶”ê°€/ì œê±°
   - ë°±ì—…/ë³µì›
   - ì—…ë°ì´íŠ¸ ì ˆì°¨
   - íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ëª¨ë“  ë¬¸ì„œ ì‘ì„± ì™„ë£Œ

#### 10.2 í”„ë¡œë•ì…˜ ë°°í¬ (2ì¼)

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ì™„ë£Œ

**Phase 10 ì™„ë£Œ ì¡°ê±´**:
- âœ… ë¬¸ì„œí™” ì™„ë£Œ
- âœ… í”„ë¡œë•ì…˜ ë°°í¬ ì™„ë£Œ

---

## ğŸ¯ ìµœì¢… ì™„ë£Œ ì¡°ê±´

### âœ… ì „ì²´ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±

- [x] **my_multihead_cluster.yaml ê¸°ë°˜ ì„¤ì •**
  - controllers ë¦¬ìŠ¤íŠ¸ë¡œ ë‹¤ì¤‘ í—¤ë“œ ì§€ì›
  - ê° controllerë§ˆë‹¤ services ì˜µì…˜
  - vip_owner ì„¤ì •

- [x] **ìë™ íƒì§€ ë° ë™ì  í´ëŸ¬ìŠ¤í„°ë§**
  - YAML íŒŒì‹±
  - í™œì„± ë…¸ë“œ ìë™ íƒì§€
  - Bootstrap vs Join ìë™ ê²°ì •
  - Nì¤‘í™” â†’ N+1ì¤‘í™” ìë™ êµ¬ì„±

- [x] **ê³µìœ  ìŠ¤í† ë¦¬ì§€ ë° ë°±ì—…**
  - /data/system_backup â†’ ë°±ì—… ì €ì¥ì†Œ
  - backup.sh â†’ ì›í´ë¦­ ë°±ì—…
  - restore.sh â†’ ì›í´ë¦­ ë³µì›

- [x] **ì™„ë²½í•œ ì¥ì•  ëŒ€ì‘**
  - 3ëŒ€ê¹Œì§€ ë™ì‹œ ë‹¤ìš´ OK
  - ìë™ VIP ì „í™˜
  - ë¬´ì¤‘ë‹¨ ì—…ë°ì´íŠ¸

### â±ï¸ ì „ì²´ ì†Œìš” ì‹œê°„: **8-10ì£¼**

| Phase | ê¸°ê°„ | ì£¼ìš” ì‘ì—… |
|-------|------|----------|
| Phase 0 | 3ì¼ | YAML íŒŒì‹± ë° ìë™ íƒì§€ |
| Phase 1 | 1ì£¼ | GlusterFS ë™ì  í´ëŸ¬ìŠ¤í„°ë§ |
| Phase 2 | 1ì£¼ | MariaDB Galera ë™ì  í´ëŸ¬ìŠ¤í„°ë§ |
| Phase 3 | 1ì£¼ | Redis Cluster ë™ì  í´ëŸ¬ìŠ¤í„°ë§ |
| Phase 4 | 1ì£¼ | Slurm Multi-Master |
| Phase 5 | 3ì¼ | Keepalived VIP ê´€ë¦¬ |
| Phase 6 | 1ì£¼ | ì›¹ ì„œë¹„ìŠ¤ í†µí•© |
| Phase 7 | 1ì£¼ | ë°±ì—…/ë³µì› ì‹œìŠ¤í…œ |
| Phase 8 | 1ì£¼ | ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± |
| Phase 9 | 2ì£¼ | í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ |
| Phase 10 | 1ì£¼ | ë¬¸ì„œí™” ë° ë°°í¬ |

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### ì²« ì„œë²„ ì‹œì‘ (Bootstrap)

```bash
cd /home/koopark/claude/KooSlurmInstallAutomationRefactory/cluster
./start_multihead.sh --config ../my_multihead_cluster.yaml --bootstrap
```

### ì¶”ê°€ ì„œë²„ ì¡°ì¸ (ìë™)

```bash
./start_multihead.sh --config ../my_multihead_cluster.yaml
â†’ ìë™ìœ¼ë¡œ ê¸°ì¡´ í´ëŸ¬ìŠ¤í„° íƒì§€ ë° ì¡°ì¸
```

### í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸

```bash
./utils/cluster_info.sh --config ../my_multihead_cluster.yaml
```

### ë°±ì—…

```bash
./backup/backup.sh --config ../my_multihead_cluster.yaml
```

### ë³µì›

```bash
./backup/restore.sh --config ../my_multihead_cluster.yaml --latest
```

---

ì´ ê³„íšì„œëŒ€ë¡œ êµ¬í˜„í•˜ë©´ **my_multihead_cluster.yaml ê¸°ë°˜ ì™„ì „ ìë™í™”ëœ ë©€í‹°í—¤ë“œ í´ëŸ¬ìŠ¤í„° ì‹œìŠ¤í…œ**ì´ ì™„ì„±ë©ë‹ˆë‹¤!
