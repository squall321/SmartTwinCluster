#!/bin/bash
################################################################################
# ì™„ì „ ìžë™í™”: Slurm 23.11.x + cgroup v2 ì „ì²´ ì„¤ì¹˜
# ì»¨íŠ¸ë¡¤ëŸ¬ + ëª¨ë“  ê³„ì‚° ë…¸ë“œ ìžë™ ì„¤ì¹˜
################################################################################

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# ì„¤ì •
COMPUTE_NODES=("192.168.122.90" "192.168.122.103")
SSH_USER="koopark"

echo "================================================================================"
echo "ðŸš€ Slurm 23.11.x + cgroup v2 ì™„ì „ ìžë™ ì„¤ì¹˜"
echo "================================================================================"
echo ""
echo "ðŸ“‹ ì„¤ì¹˜ ëŒ€ìƒ:"
echo "  ì»¨íŠ¸ë¡¤ëŸ¬: smarttwincluster (localhost)"
echo "  ê³„ì‚° ë…¸ë“œ:"
for node in "${COMPUTE_NODES[@]}"; do
    echo "    - $node"
done
echo ""

read -p "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): " -n 1 -r
echo
if [[ $REPLY =~ ^[Nn]$ ]]; then
    echo "ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
    exit 0
fi

################################################################################
# Step 1: ì»¨íŠ¸ë¡¤ëŸ¬ì— Slurm ì„¤ì¹˜
################################################################################

echo ""
echo "================================================================================"
echo "Step 1/5: ì»¨íŠ¸ë¡¤ëŸ¬ì— Slurm 23.11.x ì„¤ì¹˜"
echo "================================================================================"

chmod +x install_slurm_cgroup_v2.sh
sudo bash install_slurm_cgroup_v2.sh

if [ $? -ne 0 ]; then
    echo "âŒ ì»¨íŠ¸ë¡¤ëŸ¬ Slurm ì„¤ì¹˜ ì‹¤íŒ¨"
    exit 1
fi

echo "âœ… ì»¨íŠ¸ë¡¤ëŸ¬ Slurm ì„¤ì¹˜ ì™„ë£Œ"

################################################################################
# Step 2: ê³„ì‚° ë…¸ë“œì— Slurm ì„¤ì¹˜
################################################################################

echo ""
echo "================================================================================"
echo "Step 2/5: ê³„ì‚° ë…¸ë“œì— Slurm ì„¤ì¹˜"
echo "================================================================================"

for node in "${COMPUTE_NODES[@]}"; do
    echo ""
    echo "ðŸ“¦ $node: Slurm ì„¤ì¹˜ ì¤‘..."
    
    # ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬
    scp install_slurm_cgroup_v2.sh ${SSH_USER}@${node}:/tmp/
    
    # ì›ê²© ì‹¤í–‰
    ssh ${SSH_USER}@${node} "cd /tmp && sudo bash install_slurm_cgroup_v2.sh"
    
    if [ $? -eq 0 ]; then
        echo "âœ… $node: Slurm ì„¤ì¹˜ ì™„ë£Œ"
    else
        echo "âŒ $node: Slurm ì„¤ì¹˜ ì‹¤íŒ¨"
    fi
done

echo ""
echo "âœ… ëª¨ë“  ë…¸ë“œì— Slurm ì„¤ì¹˜ ì™„ë£Œ"

################################################################################
# Step 3: ì„¤ì • íŒŒì¼ ìƒì„± (ì»¨íŠ¸ë¡¤ëŸ¬)
################################################################################

echo ""
echo "================================================================================"
echo "Step 3/5: Slurm ì„¤ì • íŒŒì¼ ìƒì„±"
echo "================================================================================"

chmod +x configure_slurm_cgroup_v2.sh
sudo bash configure_slurm_cgroup_v2.sh

if [ $? -ne 0 ]; then
    echo "âŒ ì„¤ì • íŒŒì¼ ìƒì„± ì‹¤íŒ¨"
    exit 1
fi

echo "âœ… ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ"

################################################################################
# Step 4: ì„¤ì • íŒŒì¼ ë°°í¬ (ê³„ì‚° ë…¸ë“œ)
################################################################################

echo ""
echo "================================================================================"
echo "Step 4/5: ì„¤ì • íŒŒì¼ì„ ê³„ì‚° ë…¸ë“œì— ë°°í¬"
echo "================================================================================"

for node in "${COMPUTE_NODES[@]}"; do
    echo ""
    echo "ðŸ“¤ $node: ì„¤ì • íŒŒì¼ ë³µì‚¬ ì¤‘..."
    
    # slurm.conf ë³µì‚¬
    scp /usr/local/slurm/etc/slurm.conf ${SSH_USER}@${node}:/tmp/
    ssh ${SSH_USER}@${node} "sudo mv /tmp/slurm.conf /usr/local/slurm/etc/ && sudo chown slurm:slurm /usr/local/slurm/etc/slurm.conf"
    
    # cgroup.conf ë³µì‚¬
    scp /usr/local/slurm/etc/cgroup.conf ${SSH_USER}@${node}:/tmp/
    ssh ${SSH_USER}@${node} "sudo mv /tmp/cgroup.conf /usr/local/slurm/etc/ && sudo chown slurm:slurm /usr/local/slurm/etc/cgroup.conf"
    
    # systemd ì„œë¹„ìŠ¤ íŒŒì¼ ë³µì‚¬
    scp /etc/systemd/system/slurmd.service ${SSH_USER}@${node}:/tmp/
    ssh ${SSH_USER}@${node} "sudo mv /tmp/slurmd.service /etc/systemd/system/ && sudo systemctl daemon-reload"
    
    echo "âœ… $node: ì„¤ì • íŒŒì¼ ë³µì‚¬ ì™„ë£Œ"
done

echo ""
echo "âœ… ëª¨ë“  ë…¸ë“œì— ì„¤ì • íŒŒì¼ ë°°í¬ ì™„ë£Œ"

################################################################################
# Step 5: ì„œë¹„ìŠ¤ ì‹œìž‘
################################################################################

echo ""
echo "================================================================================"
echo "Step 5/5: Slurm ì„œë¹„ìŠ¤ ì‹œìž‘"
echo "================================================================================"

# ì»¨íŠ¸ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ ì‹œìž‘
echo ""
echo "ðŸ”§ ì»¨íŠ¸ë¡¤ëŸ¬: slurmctld ì‹œìž‘ ì¤‘..."
sudo systemctl enable slurmctld
sudo systemctl restart slurmctld

sleep 3

if sudo systemctl is-active --quiet slurmctld; then
    echo "âœ… slurmctld ì‹œìž‘ ì„±ê³µ"
else
    echo "âŒ slurmctld ì‹œìž‘ ì‹¤íŒ¨"
    sudo systemctl status slurmctld
fi

# ê³„ì‚° ë…¸ë“œ ì„œë¹„ìŠ¤ ì‹œìž‘
for node in "${COMPUTE_NODES[@]}"; do
    echo ""
    echo "ðŸ”§ $node: slurmd ì‹œìž‘ ì¤‘..."
    
    ssh ${SSH_USER}@${node} "sudo systemctl enable slurmd && sudo systemctl restart slurmd"
    
    sleep 2
    
    if ssh ${SSH_USER}@${node} "sudo systemctl is-active --quiet slurmd"; then
        echo "âœ… $node: slurmd ì‹œìž‘ ì„±ê³µ"
    else
        echo "âŒ $node: slurmd ì‹œìž‘ ì‹¤íŒ¨"
        ssh ${SSH_USER}@${node} "sudo systemctl status slurmd"
    fi
done

################################################################################
# ì™„ë£Œ ë° ê²€ì¦
################################################################################

echo ""
echo "================================================================================"
echo "ðŸŽ‰ Slurm 23.11.x + cgroup v2 ì„¤ì¹˜ ì™„ë£Œ!"
echo "================================================================================"
echo ""

sleep 5

echo "ðŸ” í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸..."
echo ""

# PATH ì„¤ì •
export PATH=/usr/local/slurm/bin:$PATH

# sinfo ì‹¤í–‰
if command -v sinfo &> /dev/null; then
    echo "ðŸ“Š ë…¸ë“œ ìƒíƒœ:"
    sinfo
    echo ""
    
    echo "ðŸ“‹ ë…¸ë“œ ìƒì„¸ ì •ë³´:"
    sinfo -N
    echo ""
else
    echo "âš ï¸  sinfo ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    echo "   PATHë¥¼ ì„¤ì •í•˜ì„¸ìš”: source /etc/profile.d/slurm.sh"
fi

echo ""
echo "================================================================================"
echo "ðŸ“‹ ë‹¤ìŒ ë‹¨ê³„"
echo "================================================================================"
echo ""
echo "1ï¸âƒ£  ë…¸ë“œê°€ DOWN ìƒíƒœë¼ë©´ í™œì„±í™”:"
echo "   /usr/local/slurm/bin/scontrol update NodeName=node001 State=RESUME"
echo "   /usr/local/slurm/bin/scontrol update NodeName=node002 State=RESUME"
echo ""
echo "2ï¸âƒ£  í…ŒìŠ¤íŠ¸ Job ì œì¶œ:"
echo "   cat > test_job.sh <<'EOF'"
echo "   #!/bin/bash"
echo "   #SBATCH --job-name=cgroup_test"
echo "   #SBATCH --output=test_%j.out"
echo "   #SBATCH --nodes=1"
echo "   #SBATCH --ntasks=1"
echo "   #SBATCH --cpus-per-task=2"
echo "   #SBATCH --mem=1G"
echo "   echo 'Hello from' \$(hostname)"
echo "   echo 'CPUs allocated:' \$SLURM_CPUS_PER_TASK"
echo "   echo 'Memory allocated:' \$SLURM_MEM_PER_NODE"
echo "   sleep 10"
echo "   EOF"
echo ""
echo "   /usr/local/slurm/bin/sbatch test_job.sh"
echo "   /usr/local/slurm/bin/squeue"
echo ""
echo "3ï¸âƒ£  cgroup v2 ìž‘ë™ í™•ì¸:"
echo "   # Job ì‹¤í–‰ ì¤‘ì— í™•ì¸"
echo "   cat /sys/fs/cgroup/system.slice/slurmstepd.scope/job_*/step_*/cgroup.procs"
echo "   cat /sys/fs/cgroup/system.slice/slurmstepd.scope/job_*/step_*/cpu.max"
echo "   cat /sys/fs/cgroup/system.slice/slurmstepd.scope/job_*/step_*/memory.max"
echo ""
echo "4ï¸âƒ£  ë¦¬ì†ŒìŠ¤ ì œí•œ í…ŒìŠ¤íŠ¸:"
echo "   # CPU ì œí•œ í…ŒìŠ¤íŠ¸"
echo "   cat > cpu_test.sh <<'EOF'"
echo "   #!/bin/bash"
echo "   #SBATCH --cpus-per-task=1"
echo "   #SBATCH --mem=512M"
echo "   stress-ng --cpu 4 --timeout 10s  # 1ê°œë§Œ í• ë‹¹í–ˆì§€ë§Œ 4ê°œ ìš”ì²­"
echo "   EOF"
echo ""
echo "   # ë©”ëª¨ë¦¬ ì œí•œ í…ŒìŠ¤íŠ¸"
echo "   cat > mem_test.sh <<'EOF'"
echo "   #!/bin/bash"
echo "   #SBATCH --mem=100M"
echo "   stress-ng --vm 1 --vm-bytes 500M --timeout 10s  # 100MBë§Œ í• ë‹¹í–ˆì§€ë§Œ 500MB ìš”ì²­"
echo "   EOF"
echo ""
echo "5ï¸âƒ£  Dashboard í†µí•©:"
echo "   cd dashboard"
echo "   export MOCK_MODE=false"
echo "   python backend/app.py &"
echo "   npm run dev"
echo ""
echo "================================================================================"
echo ""
echo "ðŸ’¡ ìœ ìš©í•œ ëª…ë ¹ì–´:"
echo ""
echo "  # Slurm ìƒíƒœ í™•ì¸"
echo "  /usr/local/slurm/bin/sinfo"
echo "  /usr/local/slurm/bin/scontrol show nodes"
echo ""
echo "  # cgroup v2 í™•ì¸"
echo "  cat /sys/fs/cgroup/cgroup.controllers"
echo "  systemd-cgls"
echo ""
echo "  # ë¡œê·¸ í™•ì¸"
echo "  sudo tail -f /var/log/slurm/slurmctld.log"
echo "  sudo tail -f /var/log/slurm/slurmd.log"
echo ""
echo "  # ì„œë¹„ìŠ¤ ìž¬ì‹œìž‘"
echo "  sudo systemctl restart slurmctld"
echo "  ssh node001 'sudo systemctl restart slurmd'"
echo ""
echo "================================================================================"
